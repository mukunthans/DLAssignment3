{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6x79REoPmV3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import wandb\n",
        "import traceback\n",
        "import os\n",
        "import io\n",
        "import csv\n",
        "import copy\n",
        "import heapq\n",
        "from IPython.display import HTML, display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- WANDB SETUP ---\n",
        "WANDB_API_KEY = \"c4c7a78b7e8600d02ded519f43e6ef09838dc431\"\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "# Special tokens\n",
        "START_TOKEN = '<'\n",
        "END_TOKEN = '>'\n",
        "PAD_TOKEN = '_'\n",
        "\n",
        "# Data paths for Tamil\n",
        "DATA_PATHS = {\n",
        "    \"train\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_train.csv\",\n",
        "    \"test\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_test.csv\",\n",
        "    \"valid\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_valid.csv\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Class for handling all data processing operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize data processor with empty dictionaries and special tokens\"\"\"\n",
        "        self.data = {\n",
        "            \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n",
        "            \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n",
        "            \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n",
        "            \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n",
        "            \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n",
        "            \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n",
        "            \"source_len\": 3,\n",
        "            \"target_len\": 3\n",
        "        }\n",
        "\n",
        "    def load_data(self, data_paths):\n",
        "        \"\"\"Load data from CSV files\"\"\"\n",
        "        data_frames = {}\n",
        "        data_pairs = {}\n",
        "\n",
        "        for split, path in data_paths.items():\n",
        "            df = pd.read_csv(path, header=None)\n",
        "            data_frames[split] = df\n",
        "            data_pairs[split] = (df[0].to_numpy(), df[1].to_numpy())\n",
        "            print(f\"Loaded {split} data: {len(df)} examples\")\n",
        "\n",
        "        return data_frames, data_pairs\n",
        "\n",
        "    def add_padding(self, sequences, max_length):\n",
        "        \"\"\"Add padding to sequences\"\"\"\n",
        "        padded_strings = []\n",
        "        for seq in sequences:\n",
        "            # Add start and end tokens\n",
        "            padded_seq = START_TOKEN + seq + END_TOKEN\n",
        "            # Truncate or pad\n",
        "            padded_seq = padded_seq[:max_length]\n",
        "            padded_seq += PAD_TOKEN * (max_length - len(padded_seq))\n",
        "            padded_strings.append(padded_seq)\n",
        "        return padded_strings\n",
        "\n",
        "    def chars_to_indices(self, string, char_index_dict):\n",
        "        \"\"\"Convert characters to their indices\"\"\"\n",
        "        char_indices = []\n",
        "        for char in string:\n",
        "            # Handle OOV characters by using PAD token index\n",
        "            if char in char_index_dict:\n",
        "                char_indices.append(char_index_dict[char])\n",
        "            else:\n",
        "                char_indices.append(char_index_dict[PAD_TOKEN])\n",
        "        return torch.tensor(char_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    def generate_sequence_from_string(self, strings, char_index_dict):\n",
        "        \"\"\"Convert strings to sequences of indices\"\"\"\n",
        "        sequences = []\n",
        "        for string in strings:\n",
        "            # Convert characters to indices\n",
        "            sequences.append(self.chars_to_indices(string, char_index_dict))\n",
        "\n",
        "        # Pad sequences to the same length\n",
        "        sequences = pad_sequence(sequences, batch_first=True, padding_value=char_index_dict[PAD_TOKEN])\n",
        "        return sequences\n",
        "\n",
        "    def update_char_dictionaries(self, padded_source, padded_target):\n",
        "        \"\"\"Update character dictionaries with new characters\"\"\"\n",
        "        for i in range(len(padded_source)):\n",
        "            for c in padded_source[i]:\n",
        "                if c not in self.data[\"source_char_index\"]:\n",
        "                    self.data[\"source_chars\"].append(c)\n",
        "                    idx = len(self.data[\"source_chars\"]) - 1\n",
        "                    self.data[\"source_char_index\"][c] = idx\n",
        "                    self.data[\"source_index_char\"][idx] = c\n",
        "\n",
        "            for c in padded_target[i]:\n",
        "                if c not in self.data[\"target_char_index\"]:\n",
        "                    self.data[\"target_chars\"].append(c)\n",
        "                    idx = len(self.data[\"target_chars\"]) - 1\n",
        "                    self.data[\"target_char_index\"][c] = idx\n",
        "                    self.data[\"target_index_char\"][idx] = c\n",
        "\n",
        "    def process_data(self, source_data, target_data):\n",
        "        \"\"\"Process source and target data\"\"\"\n",
        "        # Store original data\n",
        "        self.data[\"source_data\"] = source_data\n",
        "        self.data[\"target_data\"] = target_data\n",
        "\n",
        "        # Calculate max lengths\n",
        "        self.data[\"INPUT_MAX_LENGTH\"] = max(len(s) for s in source_data) + 2  # +2 for START and END tokens\n",
        "        self.data[\"OUTPUT_MAX_LENGTH\"] = max(len(t) for t in target_data) + 2\n",
        "\n",
        "        print(f\"Input max length: {self.data['INPUT_MAX_LENGTH']}\")\n",
        "        print(f\"Output max length: {self.data['OUTPUT_MAX_LENGTH']}\")\n",
        "\n",
        "        # Add padding\n",
        "        padded_source = self.add_padding(source_data, self.data[\"INPUT_MAX_LENGTH\"])\n",
        "        padded_target = self.add_padding(target_data, self.data[\"OUTPUT_MAX_LENGTH\"])\n",
        "\n",
        "        # Update character dictionaries\n",
        "        self.update_char_dictionaries(padded_source, padded_target)\n",
        "\n",
        "        # Generate sequences\n",
        "        self.data[\"source_data_seq\"] = self.generate_sequence_from_string(padded_source, self.data[\"source_char_index\"])\n",
        "        self.data[\"target_data_seq\"] = self.generate_sequence_from_string(padded_target, self.data[\"target_char_index\"])\n",
        "\n",
        "        # Update lengths\n",
        "        self.data[\"source_len\"] = len(self.data[\"source_chars\"])\n",
        "        self.data[\"target_len\"] = len(self.data[\"target_chars\"])\n",
        "\n",
        "        print(f\"Source vocabulary size: {self.data['source_len']}\")\n",
        "        print(f\"Target vocabulary size: {self.data['target_len']}\")\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def process_validation(self, val_source, val_target):\n",
        "        \"\"\"Process validation data using existing character maps\"\"\"\n",
        "        # Add padding\n",
        "        padded_val_source = self.add_padding(val_source, self.data[\"INPUT_MAX_LENGTH\"])\n",
        "        padded_val_target = self.add_padding(val_target, self.data[\"OUTPUT_MAX_LENGTH\"])\n",
        "\n",
        "        # Generate sequences\n",
        "        val_source_seq = self.generate_sequence_from_string(padded_val_source, self.data[\"source_char_index\"])\n",
        "        val_target_seq = self.generate_sequence_from_string(padded_val_target, self.data[\"target_char_index\"])\n",
        "\n",
        "        return val_source_seq, val_target_seq\n",
        "\n",
        "    def indices_to_string(self, indices, index_char_dict):\n",
        "        \"\"\"Convert indices to a string\"\"\"\n",
        "        string = \"\"\n",
        "        for idx in indices:\n",
        "            if isinstance(idx, torch.Tensor):\n",
        "                idx = idx.item()\n",
        "            if idx in index_char_dict:\n",
        "                char = index_char_dict[idx]\n",
        "                if char not in [PAD_TOKEN]:\n",
        "                    string += char\n",
        "        return string.replace(START_TOKEN, \"\").replace(END_TOKEN, \"\")\n",
        "\n",
        "    def prepare_input_for_prediction(self, input_string):\n",
        "        \"\"\"Prepare an input string for prediction\"\"\"\n",
        "        padded_input = START_TOKEN + input_string + END_TOKEN\n",
        "        padded_input = padded_input[:self.data[\"INPUT_MAX_LENGTH\"]]\n",
        "        padded_input += PAD_TOKEN * (self.data[\"INPUT_MAX_LENGTH\"] - len(padded_input))\n",
        "\n",
        "        # Convert to indices and create tensor\n",
        "        input_indices = [self.data[\"source_char_index\"].get(c, self.data[\"source_char_index\"][PAD_TOKEN])\n",
        "                        for c in padded_input]\n",
        "        input_tensor = torch.tensor(input_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        return input_tensor, padded_input\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"Dataset class for transliteration data\"\"\"\n",
        "    def __init__(self, source_seq, target_seq):\n",
        "        self.source_seq = source_seq\n",
        "        self.target_seq = target_seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_seq)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_seq[idx], self.target_seq[idx]\n",
        "\n",
        "class DataManager:\n",
        "    \"\"\"High-level manager for data operations\"\"\"\n",
        "    def __init__(self, data_paths):\n",
        "        self.data_paths = data_paths\n",
        "        self.processor = DataProcessor()\n",
        "\n",
        "    def load_all_data(self):\n",
        "        \"\"\"Load all data splits\"\"\"\n",
        "        _, data_pairs = self.processor.load_data(self.data_paths)\n",
        "        self.train_source, self.train_target = data_pairs[\"train\"]\n",
        "        self.val_source, self.val_target = data_pairs[\"valid\"]\n",
        "        self.test_source, self.test_target = data_pairs[\"test\"]\n",
        "\n",
        "        # Process training data\n",
        "        self.data = self.processor.process_data(self.train_source, self.train_target)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def create_dataloaders(self, h_params):\n",
        "        \"\"\"Create DataLoaders for training and validation\"\"\"\n",
        "        # Training data\n",
        "        train_dataset = TransliterationDataset(self.data[\"source_data_seq\"], self.data[\"target_data_seq\"])\n",
        "\n",
        "        # Process validation data\n",
        "        val_source_seq, val_target_seq = self.processor.process_validation(self.val_source, self.val_target)\n",
        "        val_dataset = TransliterationDataset(val_source_seq, val_target_seq)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def create_test_dataloader(self, h_params):\n",
        "        \"\"\"Create DataLoader for test data\"\"\"\n",
        "        test_source_seq, test_target_seq = self.processor.process_validation(self.test_source, self.test_target)\n",
        "        test_dataset = TransliterationDataset(test_source_seq, test_target_seq)\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        return test_loader\n",
        "\n",
        "# Class for beam search decoding\n",
        "class BeamSearchNode:\n",
        "    \"\"\"Node in beam search\"\"\"\n",
        "    def __init__(self, hidden_state, previous_node, word_id, log_prob, length, attn_weights=None):\n",
        "        self.hidden_state = hidden_state\n",
        "        self.previous_node = previous_node\n",
        "        self.word_id = word_id\n",
        "        self.log_prob = log_prob\n",
        "        self.length = length\n",
        "        self.attn_weights = attn_weights\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        \"\"\"Evaluate this node\"\"\"\n",
        "        # Normalize score by length (to avoid penalizing longer sequences)\n",
        "        return self.log_prob / float(self.length - 1 + 1e-6) ** alpha\n",
        "\n",
        "def get_chars_from_nodes(nodes, target_index_char):\n",
        "    \"\"\"Convert nodes to strings\"\"\"\n",
        "    chars = []\n",
        "    attn_weights_list = []\n",
        "\n",
        "    # Traverse backwards through the linked list of nodes\n",
        "    current = nodes[-1]\n",
        "    while current.previous_node is not None:\n",
        "        if current.word_id not in [0, 1, 2]:  # Exclude special tokens\n",
        "            chars.append(target_index_char[current.word_id])\n",
        "            if current.attn_weights is not None:\n",
        "                attn_weights_list.append(current.attn_weights)\n",
        "        current = current.previous_node\n",
        "\n",
        "    # Reverse the lists since we traversed backwards\n",
        "    chars = chars[::-1]\n",
        "    attn_weights_list = attn_weights_list[::-1] if attn_weights_list else []\n",
        "\n",
        "    return ''.join(chars), attn_weights_list"
      ],
      "metadata": {
        "id": "ytw7HT3MPxNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNType:\n",
        "    \"\"\"Enum-like class for RNN cell types\"\"\"\n",
        "    RNN = \"RNN\"\n",
        "    LSTM = \"LSTM\"\n",
        "    GRU = \"GRU\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cell(cell_type):\n",
        "        \"\"\"Get the appropriate RNN cell type\"\"\"\n",
        "        if cell_type == RNNType.RNN:\n",
        "            return nn.RNN\n",
        "        elif cell_type == RNNType.LSTM:\n",
        "            return nn.LSTM\n",
        "        elif cell_type == RNNType.GRU:\n",
        "            return nn.GRU\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism for sequence-to-sequence model\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"Forward pass for attention mechanism\"\"\"\n",
        "        # Handle LSTM hidden state\n",
        "        if isinstance(decoder_hidden, tuple):  # LSTM: (h, c)\n",
        "            decoder_hidden = decoder_hidden[0]\n",
        "\n",
        "        # Handle different shapes of decoder_hidden\n",
        "        if decoder_hidden.dim() == 3:  # (num_layers, batch, hidden_size)\n",
        "            decoder_hidden = decoder_hidden[-1].unsqueeze(1)  # (batch, 1, hidden_size)\n",
        "        elif decoder_hidden.dim() == 2:  # (batch, hidden_size)\n",
        "            decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden_size)\n",
        "        elif decoder_hidden.dim() == 1:  # (hidden_size,)\n",
        "            decoder_hidden = decoder_hidden.unsqueeze(0).unsqueeze(0)  # (1, 1, hidden_size)\n",
        "\n",
        "        # Make sure encoder_outputs has the right shape (batch, seq_len, hidden_size)\n",
        "        if encoder_outputs.dim() == 2:  # (seq_len, hidden_size)\n",
        "            encoder_outputs = encoder_outputs.unsqueeze(0)  # (1, seq_len, hidden_size)\n",
        "\n",
        "        # Check batch sizes match\n",
        "        if decoder_hidden.size(0) != encoder_outputs.size(0):\n",
        "            # Broadcast smaller batch to larger if needed\n",
        "            if decoder_hidden.size(0) == 1:\n",
        "                decoder_hidden = decoder_hidden.expand(encoder_outputs.size(0), -1, -1)\n",
        "            elif encoder_outputs.size(0) == 1:\n",
        "                encoder_outputs = encoder_outputs.expand(decoder_hidden.size(0), -1, -1)\n",
        "            else:\n",
        "                # If sizes don't match and can't broadcast, use only one example\n",
        "                decoder_hidden = decoder_hidden[:1]\n",
        "                encoder_outputs = encoder_outputs[:1]\n",
        "\n",
        "        # Calculate attention scores\n",
        "        decoder_features = self.Wa(decoder_hidden)  # (batch, 1, hidden_size)\n",
        "        encoder_features = self.Ua(encoder_outputs)  # (batch, seq_len, hidden_size)\n",
        "        energy = torch.tanh(decoder_features + encoder_features)  # (batch, seq_len, hidden_size)\n",
        "        scores = self.Va(energy)  # (batch, seq_len, 1)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=1)  # (batch, seq_len, 1)\n",
        "\n",
        "        # Calculate context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs).squeeze(1)  # (batch, hidden_size)\n",
        "\n",
        "        return context, attn_weights\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder model for sequence-to-sequence learning\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.h_params = h_params\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embed_dim\"])\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn_cell = RNNType.get_cell(h_params[\"cell_type\"])(\n",
        "            input_size=h_params[\"char_embed_dim\"],\n",
        "            hidden_size=h_params[\"hidden_size\"],\n",
        "            num_layers=h_params[\"num_layers\"],\n",
        "            dropout=h_params[\"dropout\"] if h_params[\"num_layers\"] > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(h_params[\"dropout\"])\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"Forward pass for encoder\"\"\"\n",
        "        # Embed input\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        # Pass through RNN\n",
        "        output, hidden = self.rnn_cell(embedded, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize hidden state\"\"\"\n",
        "        h = torch.zeros(self.h_params[\"num_layers\"], batch_size,\n",
        "                      self.h_params[\"hidden_size\"], device=device)\n",
        "\n",
        "        if self.h_params[\"cell_type\"] == RNNType.LSTM:\n",
        "            c = torch.zeros(self.h_params[\"num_layers\"], batch_size,\n",
        "                          self.h_params[\"hidden_size\"], device=device)\n",
        "            return (h, c)\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder model with attention for sequence-to-sequence learning\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embed_dim\"])\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention(h_params[\"hidden_size\"])\n",
        "\n",
        "        # RNN layer - input is the embedding + context vector\n",
        "        self.rnn_cell = RNNType.get_cell(h_params[\"cell_type\"])(\n",
        "            input_size=h_params[\"char_embed_dim\"] + h_params[\"hidden_size\"],\n",
        "            hidden_size=h_params[\"hidden_size\"],\n",
        "            num_layers=h_params[\"num_layers\"],\n",
        "            dropout=h_params[\"dropout\"] if h_params[\"num_layers\"] > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(h_params[\"hidden_size\"], data[\"target_len\"])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(h_params[\"dropout\"])\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        \"\"\"Forward pass for decoder with attention\"\"\"\n",
        "        # Handle different input dimensions\n",
        "        if x.dim() == 0:  # scalar\n",
        "            x = x.unsqueeze(0).unsqueeze(0)  # (1, 1)\n",
        "        elif x.dim() == 1:  # (batch,)\n",
        "            x = x.unsqueeze(1)  # (batch, 1)\n",
        "\n",
        "        # Embed input\n",
        "        embedded = self.dropout(self.embedding(x))  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        # Make sure encoder_outputs has 3 dimensions\n",
        "        if encoder_outputs.dim() == 2:\n",
        "            encoder_outputs = encoder_outputs.unsqueeze(0)  # Add batch dimension if missing\n",
        "\n",
        "        # Get attention context vector and weights\n",
        "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
        "        context = context.unsqueeze(1)  # (batch, 1, hidden_size)\n",
        "\n",
        "        # Concatenate embedding and context vector\n",
        "        rnn_input = torch.cat([embedded, context], dim=2)  # (batch, seq_len, embed_dim + hidden_size)\n",
        "\n",
        "        # Pass through RNN\n",
        "        output, hidden = self.rnn_cell(rnn_input, hidden)\n",
        "\n",
        "        # Get prediction\n",
        "        prediction = self.fc_out(output)\n",
        "\n",
        "        return F.log_softmax(prediction, dim=2), hidden, attn_weights\n",
        "\n",
        "class Seq2SeqAttentionModel:\n",
        "    \"\"\"Wrapper for encoder-decoder architecture with attention\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        self.encoder = Encoder(h_params, data).to(device)\n",
        "        self.decoder = Decoder(h_params, data).to(device)\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "\n",
        "    def forward(self, source, target=None, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"Forward pass through the model\"\"\"\n",
        "        batch_size = source.size(0)\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, encoder_hidden = self.encoder(source, encoder_hidden)\n",
        "\n",
        "        # Return early if no target (for inference)\n",
        "        if target is None:\n",
        "            return encoder_outputs, encoder_hidden\n",
        "\n",
        "        # Prepare decoder input and hidden state\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]] * batch_size, device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Determine if using teacher forcing\n",
        "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "        # Storage for decoder outputs and attention weights\n",
        "        decoder_outputs = []\n",
        "        attention_weights = []\n",
        "\n",
        "        # Decoder forward pass (one character at a time)\n",
        "        for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output.squeeze(1))  # Remove sequence dimension\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "            # Determine next input (either from target or model's prediction)\n",
        "            if use_teacher_forcing and t < self.data[\"OUTPUT_MAX_LENGTH\"] - 1:\n",
        "                decoder_input = target[:, t + 1]  # Next input is next character in target\n",
        "            else:\n",
        "                # Get predicted character\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # Next input is model's prediction\n",
        "\n",
        "        # Stack decoder outputs\n",
        "        decoder_outputs = torch.stack(decoder_outputs, dim=1)\n",
        "        attention_weights = torch.stack(attention_weights, dim=1) if attention_weights else None\n",
        "\n",
        "        return decoder_outputs, attention_weights\n",
        "\n",
        "    def greedy_decode(self, input_tensor, processor):\n",
        "        \"\"\"Perform greedy decoding with attention\"\"\"\n",
        "        # Initialize encoder\n",
        "        batch_size = input_tensor.size(0)\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "        # Prepare for decoding\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Storage for output\n",
        "        output_tokens = []\n",
        "        attn_weights_list = []\n",
        "\n",
        "        # Greedy decoding\n",
        "        for _ in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden, attn_weights = self.decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Get best prediction\n",
        "            _, topi = decoder_output.data.topk(1)\n",
        "            predicted_idx = topi.squeeze().item()\n",
        "\n",
        "            # Stop if END token\n",
        "            if predicted_idx == self.data[\"target_char_index\"][END_TOKEN]:\n",
        "                break\n",
        "\n",
        "            # Add to outputs if it's a valid character\n",
        "            if predicted_idx in self.data[\"target_index_char\"]:\n",
        "                output_tokens.append(self.data[\"target_index_char\"][predicted_idx])\n",
        "\n",
        "            # Store attention weights\n",
        "            if attn_weights is not None:\n",
        "                attn_weights_list.append(attn_weights.squeeze(0).squeeze(-1).cpu().numpy())\n",
        "\n",
        "            # Next input is predicted token\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        # Return prediction and attention weights\n",
        "        pred_str = ''.join(output_tokens)\n",
        "        attn_weights_array = np.array(attn_weights_list) if attn_weights_list else np.array([])\n",
        "\n",
        "        return pred_str, attn_weights_array\n",
        "\n",
        "    def beam_search_decode(self, input_tensor, padded_input, beam_width=5):\n",
        "        \"\"\"Perform beam search decoding with attention\"\"\"\n",
        "        # Initialize encoder\n",
        "        batch_size = input_tensor.size(0)\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "        # Number of sentences to generate\n",
        "        end_nodes = []\n",
        "\n",
        "        # Starting node\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]], device=device)\n",
        "        node = BeamSearchNode(encoder_hidden, None, decoder_input, 0, 1)\n",
        "        nodes = [(node.eval(), node)]  # Priority queue\n",
        "        heapq.heapify(nodes)\n",
        "\n",
        "        # Start beam search\n",
        "        for step in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "            # Give up when decoding takes too long\n",
        "            if len(nodes) == 0:\n",
        "                break\n",
        "\n",
        "            # Fetch the best node\n",
        "            _, current_node = heapq.heappop(nodes)\n",
        "            decoder_input = current_node.word_id\n",
        "            decoder_hidden = current_node.hidden_state\n",
        "\n",
        "            # If we reached the EOS token\n",
        "            if decoder_input.item() == self.data[\"target_char_index\"][END_TOKEN] and current_node.previous_node is not None:\n",
        "                end_nodes.append((current_node.eval(), current_node))\n",
        "                # If we have enough end nodes, stop\n",
        "                if len(end_nodes) >= beam_width:\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            # Decode for one step\n",
        "            decoder_output, decoder_hidden, attn_weights = self.decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Get top-k tokens\n",
        "            log_probs, indexes = torch.topk(decoder_output.squeeze(), beam_width)\n",
        "\n",
        "            # Put them into the queue\n",
        "            for new_k in range(beam_width):\n",
        "                decoded_t = indexes[0][new_k].view(1)\n",
        "                log_p = log_probs[0][new_k].item()\n",
        "\n",
        "                # Create new node\n",
        "                new_node = BeamSearchNode(\n",
        "                    decoder_hidden, current_node, decoded_t,\n",
        "                    current_node.log_prob + log_p, current_node.length + 1,\n",
        "                    attn_weights.cpu().numpy() if attn_weights is not None else None\n",
        "                )\n",
        "\n",
        "                # Add to priority queue\n",
        "                heapq.heappush(nodes, (new_node.eval(), new_node))\n",
        "\n",
        "        # If we don't have any end_nodes, get the top-k nodes from the queue\n",
        "        if len(end_nodes) == 0:\n",
        "            end_nodes = [heapq.heappop(nodes) for _ in range(min(beam_width, len(nodes)))]\n",
        "\n",
        "        # Sort end nodes by score (higher is better)\n",
        "        end_nodes = sorted(end_nodes, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Get the best sequence\n",
        "        best_nodes = [node for _, node in end_nodes[:beam_width]]\n",
        "\n",
        "        # Get characters and attention weights from each path\n",
        "        results = []\n",
        "        for node in best_nodes:\n",
        "            path_chars = []\n",
        "            path_attn_weights = []\n",
        "\n",
        "            # Traverse back to build sequence\n",
        "            current = node\n",
        "            node_list = []\n",
        "            while current.previous_node is not None:\n",
        "                node_list.append(current)\n",
        "                current = current.previous_node\n",
        "\n",
        "            # Reverse to get correct order\n",
        "            node_list.reverse()\n",
        "\n",
        "            # Extract characters and attention weights\n",
        "            pred_str, attn_weights_list = get_chars_from_nodes(node_list, self.data[\"target_index_char\"])\n",
        "\n",
        "            # Convert attention weights to array\n",
        "            attn_weights_array = np.array(attn_weights_list) if attn_weights_list else np.array([])\n",
        "\n",
        "            results.append((pred_str, attn_weights_array))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def predict(self, input_string, processor, use_beam_search=False, beam_width=5):\n",
        "        \"\"\"Make prediction for a single input string\"\"\"\n",
        "        # Prepare input\n",
        "        input_tensor, padded_input = processor.prepare_input_for_prediction(input_string)\n",
        "\n",
        "        # Decode\n",
        "        if use_beam_search:\n",
        "            results = self.beam_search_decode(input_tensor, padded_input, beam_width)\n",
        "            # Return best result (first in the list)\n",
        "            return results[0] if results else (\"\", np.array([]))\n",
        "        else:\n",
        "            return self.greedy_decode(input_tensor, processor)"
      ],
      "metadata": {
        "id": "DUNmZeMVP20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionVisualizer:\n",
        "    \"\"\"Class for visualizing attention weights\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_attention_matrix(input_chars, output_chars, attention_matrix, title=\"Attention Matrix\"):\n",
        "        \"\"\"Plot attention matrix as a heatmap\"\"\"\n",
        "        if attention_matrix.size == 0 or len(input_chars) == 0 or len(output_chars) == 0:\n",
        "            print(f\"No attention data available for {title}\")\n",
        "            return\n",
        "\n",
        "        # Filter out padding tokens\n",
        "        active_input_chars = [c for c in input_chars if c not in [PAD_TOKEN]]\n",
        "        active_output_chars = [c for c in output_chars if c not in [PAD_TOKEN, START_TOKEN, END_TOKEN]]\n",
        "\n",
        "        # Ensure matrix dimensions match characters\n",
        "        matrix_height = min(len(active_output_chars), attention_matrix.shape[0])\n",
        "        matrix_width = min(len(active_input_chars), attention_matrix.shape[1])\n",
        "\n",
        "        if matrix_height == 0 or matrix_width == 0:\n",
        "            print(f\"No valid dimensions for attention matrix in {title}\")\n",
        "            return\n",
        "\n",
        "        # Adjust matrix dimensions\n",
        "        adj_matrix = attention_matrix[:matrix_height, :matrix_width]\n",
        "\n",
        "        # Create plot\n",
        "        fig, ax = plt.figure(figsize=(10, 8)), plt.subplot(111)\n",
        "\n",
        "        # Plot heatmap\n",
        "        im = ax.imshow(adj_matrix, cmap='Blues')\n",
        "\n",
        "        # Add colorbar\n",
        "        plt.colorbar(im)\n",
        "\n",
        "        # Set labels\n",
        "        ax.set_xticks(range(len(active_input_chars)))\n",
        "        ax.set_yticks(range(len(active_output_chars)))\n",
        "        ax.set_xticklabels(active_input_chars, fontsize=12)\n",
        "        ax.set_yticklabels(active_output_chars, fontsize=12)\n",
        "\n",
        "        plt.xlabel('Input Sequence')\n",
        "        plt.ylabel('Output Sequence')\n",
        "        plt.title(title)\n",
        "\n",
        "        # Rotate x-axis labels for better readability\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "        # Loop over data dimensions and create text annotations\n",
        "        for i in range(matrix_height):\n",
        "            for j in range(matrix_width):\n",
        "                text = ax.text(j, i, f\"{adj_matrix[i, j]:.2f}\",\n",
        "                              ha=\"center\", va=\"center\", color=\"white\" if adj_matrix[i, j] > 0.5 else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_attention_grid(input_strings, target_strings, predictions, attention_matrices, n_samples=4):\n",
        "        \"\"\"Plot grid of attention matrices\"\"\"\n",
        "        # Calculate grid dimensions\n",
        "        n_samples = min(n_samples, len(input_strings))\n",
        "        n_cols = min(2, n_samples)\n",
        "        n_rows = (n_samples + n_cols - 1) // n_cols\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
        "\n",
        "        # Flatten axes for easier indexing if needed\n",
        "        if n_rows == 1 and n_cols == 1:\n",
        "            axes = np.array([axes])\n",
        "        elif n_rows == 1 or n_cols == 1:\n",
        "            axes = axes.flatten()\n",
        "\n",
        "        # Plot each attention matrix\n",
        "        for i in range(n_samples):\n",
        "            row, col = i // n_cols, i % n_cols\n",
        "            ax = axes[row, col] if n_rows > 1 and n_cols > 1 else axes[i]\n",
        "\n",
        "            # Get data for this sample\n",
        "            input_string = input_strings[i]\n",
        "            target_string = target_strings[i]\n",
        "            prediction = predictions[i]\n",
        "            attention_matrix = attention_matrices[i]\n",
        "\n",
        "            # Skip if no attention data\n",
        "            if attention_matrix.size == 0:\n",
        "                ax.text(0.5, 0.5, f\"No attention data for sample {i+1}\",\n",
        "                       ha='center', va='center', fontsize=12)\n",
        "                ax.axis('off')\n",
        "                continue\n",
        "\n",
        "            # Create input and output character lists\n",
        "            input_chars = [c for c in input_string if c not in [PAD_TOKEN]]\n",
        "            output_chars = [c for c in prediction if c not in [PAD_TOKEN, START_TOKEN, END_TOKEN]]\n",
        "\n",
        "            # Only use as many chars as we have attention weights for\n",
        "            input_chars = input_chars[:attention_matrix.shape[1]]\n",
        "            output_chars = output_chars[:attention_matrix.shape[0]]\n",
        "\n",
        "            # Plot the heatmap\n",
        "            im = ax.imshow(attention_matrix, cmap='Blues')\n",
        "\n",
        "            # Set labels\n",
        "            ax.set_xticks(range(len(input_chars)))\n",
        "            ax.set_yticks(range(len(output_chars)))\n",
        "            ax.set_xticklabels(input_chars, fontsize=10)\n",
        "            ax.set_yticklabels(output_chars, fontsize=10)\n",
        "\n",
        "            # Add title\n",
        "            correct = prediction == target_string\n",
        "            title = f\"Sample {i+1} - {'✓' if correct else '✗'}\\nInput: {input_string}\\nTarget: {target_string}\\nPred: {prediction}\"\n",
        "            ax.set_title(title, fontsize=10)\n",
        "\n",
        "            # Rotate x-axis labels\n",
        "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "        # Remove empty subplots\n",
        "        for i in range(n_samples, n_rows * n_cols):\n",
        "            row, col = i // n_cols, i % n_cols\n",
        "            fig.delaxes(axes[row, col] if n_rows > 1 and n_cols > 1 else axes[i])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def export_attention_html(input_chars, output_chars, attention_matrix, title=\"Attention Visualization\"):\n",
        "        \"\"\"Export attention visualization as HTML with a heatmap\"\"\"\n",
        "        if not input_chars or not output_chars or attention_matrix.size == 0:\n",
        "            return f\"<p>No attention data available for {title}</p>\"\n",
        "\n",
        "        # Filter out special tokens for better visualization\n",
        "        filtered_input_chars = [c for c in input_chars if c not in [PAD_TOKEN]]\n",
        "        filtered_output_chars = [c for c in output_chars if c not in [PAD_TOKEN, START_TOKEN, END_TOKEN]]\n",
        "\n",
        "        # Adjust attention matrix dimensions if needed\n",
        "        if len(filtered_input_chars) > 0 and len(filtered_output_chars) > 0:\n",
        "            # Ensure matrix dimensions match our characters\n",
        "            matrix_height = min(len(filtered_output_chars), attention_matrix.shape[0])\n",
        "            matrix_width = min(len(filtered_input_chars), attention_matrix.shape[1])\n",
        "\n",
        "            adj_matrix = attention_matrix[:matrix_height, :matrix_width]\n",
        "        else:\n",
        "            # Return early if no valid characters\n",
        "            return f\"<p>Not enough valid characters for {title}</p>\"\n",
        "\n",
        "        # Create a CSS-based heatmap\n",
        "        html = f\"\"\"\n",
        "        <div style=\"margin: 20px 0;\">\n",
        "            <h3>{title}</h3>\n",
        "            <div style=\"font-family: 'Noto Sans Tamil', sans-serif; margin-bottom: 10px;\">\n",
        "                <b>Input:</b> {''.join(filtered_input_chars)}<br>\n",
        "                <b>Output:</b> {''.join(filtered_output_chars)}\n",
        "            </div>\n",
        "\n",
        "            <table style=\"border-collapse: collapse; font-family: 'Noto Sans Tamil', sans-serif;\">\n",
        "                <tr>\n",
        "                    <th style=\"width: 30px;\"></th>\n",
        "                    {''.join(f'<th style=\"width: 30px; text-align: center; padding: 4px;\">{c}</th>' for c in filtered_input_chars)}\n",
        "                </tr>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add rows for each output character\n",
        "        for i, out_char in enumerate(filtered_output_chars):\n",
        "            html += f'<tr><th style=\"text-align: right; padding: 4px;\">{out_char}</th>'\n",
        "\n",
        "            # Add attention cells\n",
        "            for j, _ in enumerate(filtered_input_chars):\n",
        "                if i < adj_matrix.shape[0] and j < adj_matrix.shape[1]:\n",
        "                    # Get attention value and convert to color intensity\n",
        "                    value = adj_matrix[i, j]\n",
        "                    # Highlight the maximum value in the row\n",
        "                    is_max = value == np.max(adj_matrix[i])\n",
        "\n",
        "                    # Calculate color based on attention value (blue gradient)\n",
        "                    bg_color = f\"rgba(0, 0, 255, {value:.2f})\"\n",
        "                    border = \"2px solid red\" if is_max else \"1px solid #ddd\"\n",
        "\n",
        "                    html += f'<td style=\"width: 30px; height: 30px; background-color: {bg_color}; border: {border}; text-align: center; color: white; font-size: 12px;\">{value:.2f}</td>'\n",
        "                else:\n",
        "                    html += '<td style=\"width: 30px; height: 30px; background-color: #f9f9f9; border: 1px solid #ddd;\"></td>'\n",
        "\n",
        "            html += '</tr>'\n",
        "\n",
        "        html += '</table></div>'\n",
        "        return html\n",
        "\n",
        "    @staticmethod\n",
        "    def create_attention_grid_html(input_strings, target_strings, predictions, attention_matrices, n_samples=9):\n",
        "        \"\"\"Create HTML grid of attention visualizations\"\"\"\n",
        "        # Limit samples\n",
        "        n_samples = min(n_samples, len(input_strings))\n",
        "\n",
        "        html = \"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <style>\n",
        "                @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
        "                body {\n",
        "                    font-family: 'Noto Sans Tamil', sans-serif;\n",
        "                    font-size: 14px;\n",
        "                }\n",
        "                .grid-container {\n",
        "                    display: grid;\n",
        "                    grid-template-columns: repeat(3, 1fr);\n",
        "                    gap: 20px;\n",
        "                }\n",
        "                .attention-cell {\n",
        "                    border: 1px solid #ddd;\n",
        "                    padding: 10px;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .examples-text {\n",
        "                    margin-top: 20px;\n",
        "                    line-height: 1.6;\n",
        "                }\n",
        "                .correct { color: green; }\n",
        "                .incorrect { color: red; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h2>Attention Visualizations</h2>\n",
        "            <div class=\"grid-container\">\n",
        "        \"\"\"\n",
        "\n",
        "        input_output_pairs = []\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            input_str = input_strings[i]\n",
        "            target_str = target_strings[i]\n",
        "            pred_str = predictions[i]\n",
        "            attention_matrix = attention_matrices[i]\n",
        "\n",
        "            # Prepare character lists\n",
        "            input_chars = [c for c in input_str if c not in [PAD_TOKEN]]\n",
        "            output_chars = [c for c in pred_str if c not in [PAD_TOKEN, START_TOKEN, END_TOKEN]]\n",
        "\n",
        "            # Generate visualization HTML\n",
        "            viz_html = AttentionVisualizer.export_attention_html(\n",
        "                input_chars, output_chars, attention_matrix, f\"Example {i+1}\"\n",
        "            )\n",
        "\n",
        "            # Add to grid\n",
        "            html += f\"\"\"\n",
        "            <div class=\"attention-cell\">\n",
        "                {viz_html}\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            # Check if prediction is correct\n",
        "            correct = pred_str == target_str\n",
        "            correct_class = \"correct\" if correct else \"incorrect\"\n",
        "\n",
        "            # Save info for text list\n",
        "            input_output_pairs.append(\n",
        "                f\"Example {i+1}:<br>Input: {input_str}<br>Target: {target_str}<br>\"\n",
        "                f\"Pred: <span class='{correct_class}'>{pred_str} {'(✓)' if correct else '(✗)'}</span>\"\n",
        "            )\n",
        "\n",
        "        html += \"\"\"\n",
        "            </div>\n",
        "            <div class=\"examples-text\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Add text list\n",
        "        for text in input_output_pairs:\n",
        "            html += f\"<p>{text}</p>\"\n",
        "\n",
        "        html += \"\"\"\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        return html"
      ],
      "metadata": {
        "id": "epFZCoghP5_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    \"\"\"Class for training sequence-to-sequence models with attention\"\"\"\n",
        "    def __init__(self, model, h_params, data, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        # Initialize optimizers\n",
        "        if h_params[\"optimizer\"].lower() == \"adam\":\n",
        "            self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "        elif h_params[\"optimizer\"].lower() == \"nadam\":\n",
        "            self.encoder_optimizer = optim.NAdam(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.NAdam(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "        else:\n",
        "            self.encoder_optimizer = optim.SGD(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.SGD(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "        # Learning rate schedulers\n",
        "        self.encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.encoder_optimizer, 'min', patience=2)\n",
        "        self.decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.decoder_optimizer, 'min', patience=2)\n",
        "\n",
        "        # Best model tracking\n",
        "        self.best_val_accuracy = 0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.encoder.train()\n",
        "        self.model.decoder.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        total_correct = 0\n",
        "        total_examples = 0\n",
        "\n",
        "        for batch_idx, (source, target) in enumerate(self.train_loader):\n",
        "            batch_size = source.size(0)\n",
        "            if batch_size == 1:  # Skip batches with only one example\n",
        "                continue\n",
        "\n",
        "            # Zero gradients\n",
        "            self.encoder_optimizer.zero_grad()\n",
        "            self.decoder_optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                decoder_outputs, _ = self.model.forward(\n",
        "                    source, target, self.h_params[\"teacher_forcing_ratio\"]\n",
        "                )\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = 0\n",
        "                all_predictions = []\n",
        "\n",
        "                for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "                    decoder_output = decoder_outputs[:, t, :]\n",
        "                    target_t = target[:, t]\n",
        "                    loss += self.criterion(decoder_output, target_t)\n",
        "\n",
        "                    # Get predicted character\n",
        "                    _, topi = decoder_output.topk(1)\n",
        "                    prediction = topi.squeeze().detach()\n",
        "                    all_predictions.append(prediction)\n",
        "\n",
        "                # Combine all predictions\n",
        "                predictions = torch.stack(all_predictions, dim=1)  # batch_size x seq_len\n",
        "\n",
        "                # Calculate accuracy (exact match)\n",
        "                correct = (predictions == target).all(dim=1).sum().item()\n",
        "\n",
        "                # Update totals\n",
        "                total_correct += correct\n",
        "                total_examples += batch_size\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.encoder.parameters(), 1)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.decoder.parameters(), 1)\n",
        "\n",
        "                # Update parameters\n",
        "                self.encoder_optimizer.step()\n",
        "                self.decoder_optimizer.step()\n",
        "\n",
        "                # Track loss\n",
        "                epoch_loss += loss.item() / self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "            # Free up memory\n",
        "            del source, target, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Run garbage collection periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Print progress periodically\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {epoch_loss/(batch_idx+1):.4f}\")\n",
        "\n",
        "        if total_examples == 0:\n",
        "            return 0, 0  # Avoid division by zero if all batches were skipped\n",
        "\n",
        "        return epoch_loss / len(self.train_loader), total_correct / total_examples\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        \"\"\"Evaluate model on validation or test data\"\"\"\n",
        "        self.model.encoder.eval()\n",
        "        self.model.decoder.eval()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        total_correct = 0\n",
        "        total_examples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (source, target) in enumerate(data_loader):\n",
        "                batch_size = source.size(0)\n",
        "                if batch_size == 1:  # Skip batches with only one example\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Forward pass with teacher forcing\n",
        "                    decoder_outputs, _ = self.model.forward(\n",
        "                        source, target, teacher_forcing_ratio=1.0\n",
        "                    )\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = 0\n",
        "                    all_predictions = []\n",
        "\n",
        "                    for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "                        decoder_output = decoder_outputs[:, t, :]\n",
        "                        target_t = target[:, t]\n",
        "                        loss += self.criterion(decoder_output, target_t)\n",
        "\n",
        "                        # Get predicted character\n",
        "                        _, topi = decoder_output.topk(1)\n",
        "                        prediction = topi.squeeze().detach()\n",
        "                        all_predictions.append(prediction)\n",
        "\n",
        "                    # Combine all predictions\n",
        "                    predictions = torch.stack(all_predictions, dim=1)  # batch_size x seq_len\n",
        "\n",
        "                    # Calculate accuracy (exact match)\n",
        "                    correct = (predictions == target).all(dim=1).sum().item()\n",
        "\n",
        "                    # Update totals\n",
        "                    total_correct += correct\n",
        "                    total_examples += batch_size\n",
        "                    epoch_loss += loss.item() / self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Free up memory\n",
        "                del source, target\n",
        "\n",
        "                # Run garbage collection periodically\n",
        "                if batch_idx % 10 == 0:\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        if total_examples == 0:\n",
        "            return 0, 0  # Avoid division by zero if all batches were skipped\n",
        "\n",
        "        return total_correct / total_examples, epoch_loss / len(data_loader)\n",
        "\n",
        "    def train(self, test_source=None, test_target=None):\n",
        "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
        "        # WandB initialization\n",
        "        run_name = f\"{self.h_params['cell_type']}_{self.h_params['optimizer']}_attn_layers{self.h_params['num_layers']}\"\n",
        "        try:\n",
        "            wandb.init(project=\"Tamil-Transliteration-Attention\", name=run_name, config=self.h_params)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing wandb: {e}\")\n",
        "            print(\"Continuing without wandb tracking...\")\n",
        "\n",
        "        for epoch in range(self.h_params[\"epochs\"]):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.h_params['epochs']}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Train for one epoch\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_acc, val_loss = self.evaluate(self.val_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            self.encoder_scheduler.step(val_loss)\n",
        "            self.decoder_scheduler.step(val_loss)\n",
        "\n",
        "            # Log metrics\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            print(f\"Learning Rate: {self.encoder_optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"train_accuracy\": train_acc,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"val_accuracy\": val_acc,\n",
        "                    \"learning_rate\": self.encoder_optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_acc\n",
        "                self.save_model(f'best_model_{run_name}.pt', epoch, val_acc)\n",
        "\n",
        "            # Run garbage collection between epochs\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Test on test set if provided\n",
        "        if test_source is not None and test_target is not None:\n",
        "            self.test(test_source, test_target)\n",
        "\n",
        "        try:\n",
        "            wandb.finish()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def save_model(self, filename, epoch, val_accuracy):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        try:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'encoder_state_dict': self.model.encoder.state_dict(),\n",
        "                'decoder_state_dict': self.model.decoder.state_dict(),\n",
        "                'encoder_optimizer': self.encoder_optimizer.state_dict(),\n",
        "                'decoder_optimizer': self.decoder_optimizer.state_dict(),\n",
        "                'val_accuracy': val_accuracy,\n",
        "            }, filename)\n",
        "            print(f\"Model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model: {e}\")\n",
        "\n",
        "    def test(self, test_source, test_target):\n",
        "        \"\"\"Test model on test set\"\"\"\n",
        "        try:\n",
        "            print(\"Testing model on test set...\")\n",
        "\n",
        "            # Create DataProcessor for test data\n",
        "            processor = DataProcessor()\n",
        "            # Copy data dictionary\n",
        "            processor.data = copy.deepcopy(self.data)\n",
        "\n",
        "            # Process test data\n",
        "            test_source_seq, test_target_seq = processor.process_validation(test_source, test_target)\n",
        "            test_dataset = TransliterationDataset(test_source_seq, test_target_seq)\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=self.h_params[\"batch_size\"],\n",
        "                shuffle=False,\n",
        "                num_workers=0,\n",
        "                pin_memory=False\n",
        "            )\n",
        "\n",
        "            # Evaluate on test set\n",
        "            test_acc, test_loss = self.evaluate(test_loader)\n",
        "            print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "            # Log test metrics to wandb\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"test_loss\": test_loss,\n",
        "                    \"test_accuracy\": test_acc\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Generate predictions and compare with targets\n",
        "            evaluator = Evaluator(self.model, processor)\n",
        "\n",
        "            # Generate predictions with both greedy and beam search\n",
        "            print(\"Generating test predictions...\")\n",
        "            csv_path, greedy_results, greedy_accuracy = evaluator.generate_predictions_csv(\n",
        "                test_source, test_target, use_beam_search=False\n",
        "            )\n",
        "            print(\"Generating beam search predictions...\")\n",
        "            beam_csv_path, beam_results, beam_accuracy = evaluator.generate_predictions_csv(\n",
        "                test_source, test_target, use_beam_search=True, beam_width=5\n",
        "            )\n",
        "\n",
        "            # Compare greedy vs beam search\n",
        "            print(f\"Greedy Search Accuracy: {greedy_accuracy:.4f}\")\n",
        "            print(f\"Beam Search Accuracy: {beam_accuracy:.4f}\")\n",
        "\n",
        "            # Generate attention visualizations\n",
        "            print(\"Generating attention visualizations...\")\n",
        "            evaluator.visualize_attention(test_source, test_target, num_samples=9)\n",
        "\n",
        "            # Log final metrics to wandb\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"final_test_accuracy\": test_acc,\n",
        "                    \"final_test_loss\": test_loss,\n",
        "                    \"greedy_accuracy\": greedy_accuracy,\n",
        "                    \"beam_accuracy\": beam_accuracy\n",
        "                })\n",
        "\n",
        "                # Create a wandb Table for sample predictions\n",
        "                prediction_table = wandb.Table(columns=[\"Input\", \"Target\", \"Greedy Prediction\", \"Beam Prediction\", \"Greedy Correct\", \"Beam Correct\"])\n",
        "\n",
        "                # Include both greedy and beam search results\n",
        "                combined_results = []\n",
        "                for i in range(min(100, len(greedy_results))):\n",
        "                    if i < len(beam_results):\n",
        "                        input_str = greedy_results[i]['Input']\n",
        "                        target_str = greedy_results[i]['Target']\n",
        "                        greedy_pred = greedy_results[i]['Prediction']\n",
        "                        beam_pred = beam_results[i]['Prediction']\n",
        "                        greedy_correct = greedy_pred == target_str\n",
        "                        beam_correct = beam_pred == target_str\n",
        "\n",
        "                        prediction_table.add_data(\n",
        "                            input_str, target_str,\n",
        "                            greedy_pred, beam_pred,\n",
        "                            greedy_correct, beam_correct\n",
        "                        )\n",
        "\n",
        "                        combined_results.append({\n",
        "                            'Input': input_str,\n",
        "                            'Target': target_str,\n",
        "                            'Greedy': greedy_pred,\n",
        "                            'Beam': beam_pred\n",
        "                        })\n",
        "\n",
        "                wandb.log({\"prediction_samples\": prediction_table})\n",
        "\n",
        "                # Log attention visualizations as images\n",
        "                evaluator.log_attention_visualizations_to_wandb(num_samples=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not log to wandb: {e}\")\n",
        "\n",
        "            # Display sample predictions\n",
        "            evaluator.compare_decoding_methods(test_source, test_target, num_samples=5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in testing: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"Class for evaluating transliteration models with attention\"\"\"\n",
        "    def __init__(self, model, processor):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.data = processor.data\n",
        "        self.visualizer = AttentionVisualizer()\n",
        "\n",
        "    def predict(self, input_string, use_beam_search=False, beam_width=5):\n",
        "        \"\"\"Make prediction for a single input string\"\"\"\n",
        "        return self.model.predict(input_string, self.processor, use_beam_search, beam_width)\n",
        "\n",
        "    def generate_predictions_csv(self, test_source, test_target, use_beam_search=False, beam_width=5):\n",
        "        \"\"\"Generate predictions for all test data and save to CSV\"\"\"\n",
        "        print(f\"Generating predictions for {len(test_source)} test examples...\")\n",
        "\n",
        "        results = []\n",
        "        attention_matrices = []\n",
        "\n",
        "        # Generate predictions\n",
        "        for i in range(len(test_source)):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Processing test example {i+1}/{len(test_source)}\")\n",
        "\n",
        "            input_str = test_source[i]\n",
        "            target_str = test_target[i]\n",
        "\n",
        "            # Get prediction with attention weights\n",
        "            if use_beam_search:\n",
        "                result = self.predict(input_str, use_beam_search=True, beam_width=beam_width)\n",
        "                pred_str, attn_matrix = result\n",
        "            else:\n",
        "                pred_str, attn_matrix = self.predict(input_str, use_beam_search=False)\n",
        "\n",
        "            # Store result\n",
        "            results.append({\n",
        "                'Input': input_str,\n",
        "                'Prediction': pred_str,\n",
        "                'Target': target_str\n",
        "            })\n",
        "\n",
        "            # Store attention matrix\n",
        "            attention_matrices.append(attn_matrix)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = sum(1 for r in results if r['Prediction'] == r['Target'])\n",
        "        accuracy = correct / len(results) if results else 0\n",
        "\n",
        "        method = \"beam_search\" if use_beam_search else \"greedy\"\n",
        "        print(f\"{method.capitalize()} Accuracy: {accuracy:.4f} ({correct}/{len(results)})\")\n",
        "\n",
        "        # Add accuracy to wandb\n",
        "        try:\n",
        "            wandb.log({f\"{method}_accuracy\": accuracy})\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Save to CSV\n",
        "        csv_path = f'tamil_transliteration_predictions_{method}.csv'\n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['Input', 'Prediction', 'Target']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for row in results:\n",
        "                writer.writerow(row)\n",
        "\n",
        "        print(f\"Saved predictions to {csv_path}\")\n",
        "\n",
        "        # Store attention matrices for later use\n",
        "        self.last_results = results\n",
        "        self.last_attention_matrices = attention_matrices\n",
        "\n",
        "        return csv_path, results, accuracy\n",
        "\n",
        "    def visualize_attention(self, test_source, test_target, num_samples=9):\n",
        "        \"\"\"Visualize attention weights for sample predictions\"\"\"\n",
        "        # Select random samples\n",
        "        indices = np.random.choice(len(test_source), min(num_samples, len(test_source)), replace=False)\n",
        "\n",
        "        input_strings = []\n",
        "        target_strings = []\n",
        "        predictions = []\n",
        "        attention_matrices = []\n",
        "\n",
        "        for idx in indices:\n",
        "            input_str = test_source[idx]\n",
        "            target_str = test_target[idx]\n",
        "\n",
        "            # Get prediction with attention\n",
        "            pred_str, attn_matrix = self.predict(input_str)\n",
        "\n",
        "            input_strings.append(input_str)\n",
        "            target_strings.append(target_str)\n",
        "            predictions.append(pred_str)\n",
        "            attention_matrices.append(attn_matrix)\n",
        "\n",
        "        # Create HTML grid\n",
        "        html = self.visualizer.create_attention_grid_html(\n",
        "            input_strings, target_strings, predictions, attention_matrices\n",
        "        )\n",
        "\n",
        "        # Save HTML file\n",
        "        with open(\"attention_visualizations.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html)\n",
        "\n",
        "        # Display in notebook if possible\n",
        "        try:\n",
        "            display(HTML(html))\n",
        "            print(\"Attention visualizations displayed in notebook and saved to attention_visualizations.html\")\n",
        "        except:\n",
        "            print(\"Attention visualizations saved to attention_visualizations.html\")\n",
        "\n",
        "        # Also plot as matplotlib figures\n",
        "        fig = self.visualizer.plot_attention_grid(\n",
        "            input_strings, target_strings, predictions, attention_matrices, n_samples=4\n",
        "        )\n",
        "\n",
        "        # Save figure\n",
        "        try:\n",
        "            fig.savefig(\"attention_grid.png\", dpi=150, bbox_inches='tight')\n",
        "            print(\"Attention grid saved to attention_grid.png\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving attention grid: {e}\")\n",
        "\n",
        "        # Save results for later use\n",
        "        self.vis_input_strings = input_strings\n",
        "        self.vis_target_strings = target_strings\n",
        "        self.vis_predictions = predictions\n",
        "        self.vis_attention_matrices = attention_matrices\n",
        "\n",
        "        return html\n",
        "\n",
        "    def compare_decoding_methods(self, test_source, test_target, num_samples=5):\n",
        "        \"\"\"Compare greedy and beam search decoding\"\"\"\n",
        "        # Select random samples\n",
        "        indices = np.random.choice(len(test_source), min(num_samples, len(test_source)), replace=False)\n",
        "\n",
        "        comparison_results = []\n",
        "\n",
        "        for idx in indices:\n",
        "            input_str = test_source[idx]\n",
        "            target_str = test_target[idx]\n",
        "\n",
        "            # Get predictions using both methods\n",
        "            greedy_pred, _ = self.predict(input_str, use_beam_search=False)\n",
        "            beam_result = self.predict(input_str, use_beam_search=True, beam_width=5)\n",
        "            beam_pred = beam_result[0] if isinstance(beam_result, tuple) else beam_result\n",
        "\n",
        "            comparison_results.append({\n",
        "                'Input': input_str,\n",
        "                'Target': target_str,\n",
        "                'Greedy': greedy_pred,\n",
        "                'Beam': beam_pred\n",
        "            })\n",
        "\n",
        "        # Create HTML table\n",
        "        html = \"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <style>\n",
        "                @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
        "                body {\n",
        "                    font-family: 'Noto Sans Tamil', sans-serif;\n",
        "                    font-size: 16px;\n",
        "                    line-height: 1.6;\n",
        "                }\n",
        "                table {\n",
        "                    border-collapse: collapse;\n",
        "                    width: 100%;\n",
        "                    margin: 20px 0;\n",
        "                }\n",
        "                th, td {\n",
        "                    padding: 12px;\n",
        "                    text-align: left;\n",
        "                    border: 1px solid #ddd;\n",
        "                }\n",
        "                th {\n",
        "                    background-color: #f2f2f2;\n",
        "                    font-weight: bold;\n",
        "                }\n",
        "                tr:nth-child(even) {\n",
        "                    background-color: #f9f9f9;\n",
        "                }\n",
        "                .correct { color: green; }\n",
        "                .incorrect { color: red; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h2>Greedy vs Beam Search Decoding Comparison</h2>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Input</th>\n",
        "                    <th>Target</th>\n",
        "                    <th>Greedy Prediction</th>\n",
        "                    <th>Beam Search Prediction</th>\n",
        "                </tr>\n",
        "        \"\"\"\n",
        "\n",
        "        for result in comparison_results:\n",
        "            greedy_correct = result['Greedy'] == result['Target']\n",
        "            beam_correct = result['Beam'] == result['Target']\n",
        "\n",
        "            greedy_class = \"correct\" if greedy_correct else \"incorrect\"\n",
        "            beam_class = \"correct\" if beam_correct else \"incorrect\"\n",
        "\n",
        "            html += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{result['Input']}</td>\n",
        "                <td>{result['Target']}</td>\n",
        "                <td class=\"{greedy_class}\">{result['Greedy']} {\"✓\" if greedy_correct else \"✗\"}</td>\n",
        "                <td class=\"{beam_class}\">{result['Beam']} {\"✓\" if beam_correct else \"✗\"}</td>\n",
        "            </tr>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"\"\"\n",
        "            </table>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Display in notebook if possible\n",
        "        try:\n",
        "            display(HTML(html))\n",
        "        except:\n",
        "            # Fallback\n",
        "            print(\"HTML display not available. Showing text comparison:\")\n",
        "            for result in comparison_results:\n",
        "                print(f\"Input: {result['Input']}\")\n",
        "                print(f\"Target: {result['Target']}\")\n",
        "                print(f\"Greedy: {result['Greedy']} {'✓' if result['Greedy'] == result['Target'] else '✗'}\")\n",
        "                print(f\"Beam: {result['Beam']} {'✓' if result['Beam'] == result['Target'] else '✗'}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    def log_attention_visualizations_to_wandb(self, num_samples=4):\n",
        "        \"\"\"Log attention visualizations to wandb\"\"\"\n",
        "        try:\n",
        "            if not hasattr(self, 'vis_input_strings'):\n",
        "                # No visualizations generated yet\n",
        "                return\n",
        "\n",
        "            # Create matplotlib figures\n",
        "            fig = self.visualizer.plot_attention_grid(\n",
        "                self.vis_input_strings[:num_samples],\n",
        "                self.vis_target_strings[:num_samples],\n",
        "                self.vis_predictions[:num_samples],\n",
        "                self.vis_attention_matrices[:num_samples],\n",
        "                n_samples=num_samples\n",
        "            )\n",
        "\n",
        "            # Log to wandb\n",
        "            wandb.log({\"attention_grid\": wandb.Image(fig)})\n",
        "\n",
        "            # Create individual attention matrices\n",
        "            for i in range(min(num_samples, len(self.vis_input_strings))):\n",
        "                if self.vis_attention_matrices[i].size > 0:\n",
        "                    input_chars = [c for c in self.vis_input_strings[i] if c not in [PAD_TOKEN]]\n",
        "                    output_chars = [c for c in self.vis_predictions[i] if c not in [PAD_TOKEN, START_TOKEN, END_TOKEN]]\n",
        "\n",
        "                    # Create individual attention visualization\n",
        "                    fig_i = self.visualizer.plot_attention_matrix(\n",
        "                        input_chars, output_chars, self.vis_attention_matrices[i],\n",
        "                        f\"Sample {i+1}: {self.vis_input_strings[i]} → {self.vis_predictions[i]}\"\n",
        "                    )\n",
        "\n",
        "                    if fig_i:\n",
        "                        wandb.log({f\"attention_matrix_{i+1}\": wandb.Image(fig_i)})\n",
        "\n",
        "            print(\"Attention visualizations logged to wandb\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error logging attention visualizations to wandb: {e}\")"
      ],
      "metadata": {
        "id": "28Jir46gP9nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperparameterTuner:\n",
        "    \"\"\"Class for tuning hyperparameters using wandb sweeps\"\"\"\n",
        "    def __init__(self, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "\n",
        "    def get_sweep_config(self):\n",
        "        \"\"\"Define hyperparameter sweep configuration\"\"\"\n",
        "        sweep_config = {\n",
        "            'method': 'bayes',\n",
        "            'name': 'tamil-transliteration-attention-sweep',\n",
        "            'metric': {\n",
        "                'goal': 'maximize',\n",
        "                'name': 'val_accuracy'\n",
        "            },\n",
        "            'parameters': {\n",
        "                'learning_rate': {\n",
        "                    'values': [0.001, 0.0005, 0.0001]\n",
        "                },\n",
        "                'batch_size': {\n",
        "                    'values': [32, 64, 128, 256]\n",
        "                },\n",
        "                'char_embed_dim': {\n",
        "                    'values': [64, 128, 256]\n",
        "                },\n",
        "                'hidden_size': {\n",
        "                    'values': [128, 256, 512]\n",
        "                },\n",
        "                'num_layers': {\n",
        "                    'values': [1, 2, 3, 4]\n",
        "                },\n",
        "                'cell_type': {\n",
        "                    'values': [RNNType.RNN, RNNType.LSTM, RNNType.GRU]\n",
        "                },\n",
        "                'dropout': {\n",
        "                    'values': [0.0, 0.1, 0.2, 0.3]\n",
        "                },\n",
        "                'optimizer': {\n",
        "                    'values': ['adam', 'nadam']\n",
        "                },\n",
        "                'epochs': {\n",
        "                    'values': [10, 15, 20]\n",
        "                },\n",
        "                'teacher_forcing_ratio': {\n",
        "                    'values': [0.3, 0.5, 0.7]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return sweep_config\n",
        "\n",
        "    def sweep_agent(self):\n",
        "        \"\"\"Sweep agent function\"\"\"\n",
        "        wandb.init()\n",
        "\n",
        "        # Access sweep config\n",
        "        config = wandb.config\n",
        "\n",
        "        # Define hyperparameters\n",
        "        h_params = {\n",
        "            \"char_embed_dim\": config.char_embed_dim,\n",
        "            \"hidden_size\": config.hidden_size,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"num_layers\": config.num_layers,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"epochs\": config.epochs,\n",
        "            \"cell_type\": config.cell_type,\n",
        "            \"dropout\": config.dropout,\n",
        "            \"optimizer\": config.optimizer,\n",
        "            \"teacher_forcing_ratio\": config.teacher_forcing_ratio\n",
        "        }\n",
        "\n",
        "        # Load data\n",
        "        try:\n",
        "            # Process data\n",
        "            data = self.data_manager.load_all_data()\n",
        "\n",
        "            # Create dataloaders\n",
        "            train_loader, val_loader = self.data_manager.create_dataloaders(h_params)\n",
        "\n",
        "            # Create model\n",
        "            model = Seq2SeqAttentionModel(h_params, data)\n",
        "\n",
        "            # Train model\n",
        "            trainer = Trainer(model, h_params, data, train_loader, val_loader)\n",
        "            trainer.train(self.data_manager.test_source, self.data_manager.test_target)\n",
        "\n",
        "            # Close wandb run\n",
        "            wandb.finish()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sweep agent: {e}\")\n",
        "            traceback.print_exc()\n",
        "            wandb.finish()\n",
        "\n",
        "    def run_sweep(self, count=10):\n",
        "        \"\"\"Run hyperparameter sweep\"\"\"\n",
        "        sweep_config = self.get_sweep_config()\n",
        "        sweep_id = wandb.sweep(sweep=sweep_config, project=\"Tamil-Transliteration-Attention\")\n",
        "        wandb.agent(sweep_id, function=self.sweep_agent, count=count)"
      ],
      "metadata": {
        "id": "6mc8VWGUQXNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment:\n",
        "    \"\"\"Class for running experiments\"\"\"\n",
        "    def __init__(self):\n",
        "        # Default hyperparameters\n",
        "        self.default_hyperparams = {\n",
        "            \"char_embed_dim\": 256,\n",
        "            \"hidden_size\": 256,\n",
        "            \"batch_size\": 256,\n",
        "            \"num_layers\": 2,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"epochs\": 15,\n",
        "            \"cell_type\": RNNType.GRU,\n",
        "            \"dropout\": 0.2,\n",
        "            \"optimizer\": \"nadam\",\n",
        "            \"teacher_forcing_ratio\": 0.5\n",
        "        }\n",
        "\n",
        "    def run(self, use_sweep=False, num_sweep_runs=10, use_beam_search=True):\n",
        "        \"\"\"Run experiment\"\"\"\n",
        "        try:\n",
        "            # Initial CUDA memory reset\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Create data manager\n",
        "            data_manager = DataManager(DATA_PATHS)\n",
        "\n",
        "            if use_sweep:\n",
        "                # Run hyperparameter sweep\n",
        "                tuner = HyperparameterTuner(data_manager)\n",
        "                tuner.run_sweep(count=num_sweep_runs)\n",
        "            else:\n",
        "                # Load data\n",
        "                data = data_manager.load_all_data()\n",
        "\n",
        "                # Create dataloaders\n",
        "                train_loader, val_loader = data_manager.create_dataloaders(self.default_hyperparams)\n",
        "\n",
        "                # Create model\n",
        "                model = Seq2SeqAttentionModel(self.default_hyperparams, data)\n",
        "\n",
        "                # Initialize wandb\n",
        "                run_name = f\"{self.default_hyperparams['cell_type']}_{self.default_hyperparams['optimizer']}_attn_layers{self.default_hyperparams['num_layers']}\"\n",
        "                try:\n",
        "                    wandb.init(project=\"Tamil-Transliteration-Attention\", name=run_name, config=self.default_hyperparams)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error initializing wandb: {e}\")\n",
        "\n",
        "                # Train model\n",
        "                print(f\"Starting training with {self.default_hyperparams['cell_type']} cell, {self.default_hyperparams['optimizer']} optimizer (With Attention)\")\n",
        "                trainer = Trainer(model, self.default_hyperparams, data, train_loader, val_loader)\n",
        "                trainer.train(data_manager.test_source, data_manager.test_target)\n",
        "\n",
        "                # Test comparison between greedy and beam search if requested\n",
        "                if use_beam_search and model is not None:\n",
        "                    print(\"\\nComparing greedy vs beam search decoding...\")\n",
        "                    processor = DataProcessor()\n",
        "                    processor.data = data\n",
        "                    evaluator = Evaluator(model, processor)\n",
        "\n",
        "                    # Compare greedy vs beam search results\n",
        "                    evaluator.compare_decoding_methods(data_manager.test_source, data_manager.test_target, num_samples=10)\n",
        "\n",
        "                    # Visualize attention patterns\n",
        "                    print(\"\\nVisualizing attention patterns...\")\n",
        "                    evaluator.visualize_attention(data_manager.test_source, data_manager.test_target, num_samples=6)\n",
        "\n",
        "                # Close wandb run\n",
        "                try:\n",
        "                    wandb.finish()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in experiment: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Run experiment\n",
        "def main():\n",
        "    experiment = Experiment()\n",
        "\n",
        "    # Set use_sweep=True to run hyperparameter sweep, False to run with default hyperparameters\n",
        "    # Set use_beam_search=True to test beam search decoding\n",
        "    experiment.run(use_sweep=False, use_beam_search=True)\n",
        "\n",
        "    # Uncomment to run sweep with 20 runs\n",
        "    # experiment.run(use_sweep=True, num_sweep_runs=20)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dYszMO-IQjlh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}