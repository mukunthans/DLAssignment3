{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm7imoHiN4gH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import gc\n",
        "import wandb\n",
        "import traceback\n",
        "import os\n",
        "import io\n",
        "import csv\n",
        "import copy\n",
        "import heapq\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- WANDB SETUP ---\n",
        "WANDB_API_KEY = \"c4c7a78b7e8600d02ded519f43e6ef09838dc431\"\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "# Special tokens\n",
        "START_TOKEN = '<'\n",
        "END_TOKEN = '>'\n",
        "PAD_TOKEN = '_'\n",
        "\n",
        "# Teacher forcing ratio\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "\n",
        "# Data paths for Tamil\n",
        "DATA_PATHS = {\n",
        "    \"train\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_train.csv\",\n",
        "    \"test\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_test.csv\",\n",
        "    \"valid\": \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/tam/tam_valid.csv\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Class for handling all data processing operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize data processor with empty dictionaries and special tokens\"\"\"\n",
        "        self.data = {\n",
        "            \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n",
        "            \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n",
        "            \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n",
        "            \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n",
        "            \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n",
        "            \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n",
        "            \"source_len\": 3,\n",
        "            \"target_len\": 3\n",
        "        }\n",
        "\n",
        "    def load_data(self, data_paths):\n",
        "        \"\"\"Load data from CSV files\"\"\"\n",
        "        data_frames = {}\n",
        "        data_pairs = {}\n",
        "\n",
        "        for split, path in data_paths.items():\n",
        "            df = pd.read_csv(path, header=None)\n",
        "            data_frames[split] = df\n",
        "            data_pairs[split] = (df[0].to_numpy(), df[1].to_numpy())\n",
        "            print(f\"Loaded {split} data: {len(df)} examples\")\n",
        "\n",
        "        return data_frames, data_pairs\n",
        "\n",
        "    def add_padding(self, sequences, max_length):\n",
        "        \"\"\"Add padding to sequences\"\"\"\n",
        "        padded_strings = []\n",
        "        for seq in sequences:\n",
        "            # Add start and end tokens\n",
        "            padded_seq = START_TOKEN + seq + END_TOKEN\n",
        "            # Truncate or pad\n",
        "            padded_seq = padded_seq[:max_length]\n",
        "            padded_seq += PAD_TOKEN * (max_length - len(padded_seq))\n",
        "            padded_strings.append(padded_seq)\n",
        "        return padded_strings\n",
        "\n",
        "    def chars_to_indices(self, string, char_index_dict):\n",
        "        \"\"\"Convert characters to their indices\"\"\"\n",
        "        char_indices = []\n",
        "        for char in string:\n",
        "            # Handle OOV characters by using PAD token index\n",
        "            if char in char_index_dict:\n",
        "                char_indices.append(char_index_dict[char])\n",
        "            else:\n",
        "                char_indices.append(char_index_dict[PAD_TOKEN])\n",
        "        return torch.tensor(char_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    def generate_sequence_from_string(self, strings, char_index_dict):\n",
        "        \"\"\"Convert strings to sequences of indices\"\"\"\n",
        "        sequences = []\n",
        "        for string in strings:\n",
        "            # Convert characters to indices\n",
        "            sequences.append(self.chars_to_indices(string, char_index_dict))\n",
        "\n",
        "        # Pad sequences to the same length\n",
        "        sequences = pad_sequence(sequences, batch_first=True, padding_value=char_index_dict[PAD_TOKEN])\n",
        "        return sequences\n",
        "\n",
        "    def update_char_dictionaries(self, padded_source, padded_target):\n",
        "        \"\"\"Update character dictionaries with new characters\"\"\"\n",
        "        for i in range(len(padded_source)):\n",
        "            for c in padded_source[i]:\n",
        "                if c not in self.data[\"source_char_index\"]:\n",
        "                    self.data[\"source_chars\"].append(c)\n",
        "                    idx = len(self.data[\"source_chars\"]) - 1\n",
        "                    self.data[\"source_char_index\"][c] = idx\n",
        "                    self.data[\"source_index_char\"][idx] = c\n",
        "\n",
        "            for c in padded_target[i]:\n",
        "                if c not in self.data[\"target_char_index\"]:\n",
        "                    self.data[\"target_chars\"].append(c)\n",
        "                    idx = len(self.data[\"target_chars\"]) - 1\n",
        "                    self.data[\"target_char_index\"][c] = idx\n",
        "                    self.data[\"target_index_char\"][idx] = c\n",
        "\n",
        "    def process_data(self, source_data, target_data):\n",
        "        \"\"\"Process source and target data\"\"\"\n",
        "        # Store original data\n",
        "        self.data[\"source_data\"] = source_data\n",
        "        self.data[\"target_data\"] = target_data\n",
        "\n",
        "        # Calculate max lengths\n",
        "        self.data[\"INPUT_MAX_LENGTH\"] = max(len(s) for s in source_data) + 2  # +2 for START and END tokens\n",
        "        self.data[\"OUTPUT_MAX_LENGTH\"] = max(len(t) for t in target_data) + 2\n",
        "\n",
        "        print(f\"Input max length: {self.data['INPUT_MAX_LENGTH']}\")\n",
        "        print(f\"Output max length: {self.data['OUTPUT_MAX_LENGTH']}\")\n",
        "\n",
        "        # Add padding\n",
        "        padded_source = self.add_padding(source_data, self.data[\"INPUT_MAX_LENGTH\"])\n",
        "        padded_target = self.add_padding(target_data, self.data[\"OUTPUT_MAX_LENGTH\"])\n",
        "\n",
        "        # Update character dictionaries\n",
        "        self.update_char_dictionaries(padded_source, padded_target)\n",
        "\n",
        "        # Generate sequences\n",
        "        self.data[\"source_data_seq\"] = self.generate_sequence_from_string(padded_source, self.data[\"source_char_index\"])\n",
        "        self.data[\"target_data_seq\"] = self.generate_sequence_from_string(padded_target, self.data[\"target_char_index\"])\n",
        "\n",
        "        # Update lengths\n",
        "        self.data[\"source_len\"] = len(self.data[\"source_chars\"])\n",
        "        self.data[\"target_len\"] = len(self.data[\"target_chars\"])\n",
        "\n",
        "        print(f\"Source vocabulary size: {self.data['source_len']}\")\n",
        "        print(f\"Target vocabulary size: {self.data['target_len']}\")\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def process_validation(self, val_source, val_target):\n",
        "        \"\"\"Process validation data using existing character maps\"\"\"\n",
        "        # Add padding\n",
        "        padded_val_source = self.add_padding(val_source, self.data[\"INPUT_MAX_LENGTH\"])\n",
        "        padded_val_target = self.add_padding(val_target, self.data[\"OUTPUT_MAX_LENGTH\"])\n",
        "\n",
        "        # Generate sequences\n",
        "        val_source_seq = self.generate_sequence_from_string(padded_val_source, self.data[\"source_char_index\"])\n",
        "        val_target_seq = self.generate_sequence_from_string(padded_val_target, self.data[\"target_char_index\"])\n",
        "\n",
        "        return val_source_seq, val_target_seq\n",
        "\n",
        "    def indices_to_string(self, indices, index_char_dict):\n",
        "        \"\"\"Convert indices to a string\"\"\"\n",
        "        string = \"\"\n",
        "        for idx in indices:\n",
        "            if isinstance(idx, torch.Tensor):\n",
        "                idx = idx.item()\n",
        "            if idx in index_char_dict:\n",
        "                char = index_char_dict[idx]\n",
        "                if char not in [PAD_TOKEN]:\n",
        "                    string += char\n",
        "        return string.replace(START_TOKEN, \"\").replace(END_TOKEN, \"\")\n",
        "\n",
        "    def prepare_input_for_prediction(self, input_string):\n",
        "        \"\"\"Prepare an input string for prediction\"\"\"\n",
        "        padded_input = START_TOKEN + input_string + END_TOKEN\n",
        "        padded_input = padded_input[:self.data[\"INPUT_MAX_LENGTH\"]]\n",
        "        padded_input += PAD_TOKEN * (self.data[\"INPUT_MAX_LENGTH\"] - len(padded_input))\n",
        "\n",
        "        # Convert to indices and create tensor\n",
        "        input_indices = [self.data[\"source_char_index\"].get(c, self.data[\"source_char_index\"][PAD_TOKEN])\n",
        "                        for c in padded_input]\n",
        "        input_tensor = torch.tensor(input_indices, device=device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        return input_tensor\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    \"\"\"Dataset class for transliteration data\"\"\"\n",
        "    def __init__(self, source_seq, target_seq):\n",
        "        self.source_seq = source_seq\n",
        "        self.target_seq = target_seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_seq)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_seq[idx], self.target_seq[idx]\n",
        "\n",
        "class DataManager:\n",
        "    \"\"\"High-level manager for data operations\"\"\"\n",
        "    def __init__(self, data_paths):\n",
        "        self.data_paths = data_paths\n",
        "        self.processor = DataProcessor()\n",
        "\n",
        "    def load_all_data(self):\n",
        "        \"\"\"Load all data splits\"\"\"\n",
        "        _, data_pairs = self.processor.load_data(self.data_paths)\n",
        "        self.train_source, self.train_target = data_pairs[\"train\"]\n",
        "        self.val_source, self.val_target = data_pairs[\"valid\"]\n",
        "        self.test_source, self.test_target = data_pairs[\"test\"]\n",
        "\n",
        "        # Process training data\n",
        "        self.data = self.processor.process_data(self.train_source, self.train_target)\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def create_dataloaders(self, h_params):\n",
        "        \"\"\"Create DataLoaders for training and validation\"\"\"\n",
        "        # Training data\n",
        "        train_dataset = TransliterationDataset(self.data[\"source_data_seq\"], self.data[\"target_data_seq\"])\n",
        "\n",
        "        # Process validation data\n",
        "        val_source_seq, val_target_seq = self.processor.process_validation(self.val_source, self.val_target)\n",
        "        val_dataset = TransliterationDataset(val_source_seq, val_target_seq)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def create_test_dataloader(self, h_params):\n",
        "        \"\"\"Create DataLoader for test data\"\"\"\n",
        "        test_source_seq, test_target_seq = self.processor.process_validation(self.test_source, self.test_target)\n",
        "        test_dataset = TransliterationDataset(test_source_seq, test_target_seq)\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=h_params[\"batch_size\"],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        return test_loader\n",
        "\n",
        "# Class for beam search decoding\n",
        "class BeamSearchNode:\n",
        "    \"\"\"Node in beam search\"\"\"\n",
        "    def __init__(self, hidden_state, previous_node, word_id, log_prob, length):\n",
        "        self.hidden_state = hidden_state\n",
        "        self.previous_node = previous_node\n",
        "        self.word_id = word_id\n",
        "        self.log_prob = log_prob\n",
        "        self.length = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        \"\"\"Evaluate this node\"\"\"\n",
        "        # Normalize score by length (to avoid penalizing longer sequences)\n",
        "        return self.log_prob / float(self.length - 1 + 1e-6) ** alpha\n",
        "\n",
        "def get_chars_string_from_nodes(nodes, target_index_char):\n",
        "    \"\"\"Convert nodes to strings\"\"\"\n",
        "    chars = []\n",
        "    for n in nodes:\n",
        "        if n.word_id != 0 and n.word_id != 1 and n.word_id != 2:  # Exclude special tokens\n",
        "            chars.append(target_index_char[n.word_id])\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "Dj_utvBcN6t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNType:\n",
        "    \"\"\"Enum-like class for RNN cell types\"\"\"\n",
        "    RNN = \"RNN\"\n",
        "    LSTM = \"LSTM\"\n",
        "    GRU = \"GRU\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cell(cell_type):\n",
        "        \"\"\"Get the appropriate RNN cell type\"\"\"\n",
        "        if cell_type == RNNType.RNN:\n",
        "            return nn.RNN\n",
        "        elif cell_type == RNNType.LSTM:\n",
        "            return nn.LSTM\n",
        "        elif cell_type == RNNType.GRU:\n",
        "            return nn.GRU\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder model for sequence-to-sequence learning\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.h_params = h_params\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embed_dim\"])\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn_cell = RNNType.get_cell(h_params[\"cell_type\"])(\n",
        "            input_size=h_params[\"char_embed_dim\"],\n",
        "            hidden_size=h_params[\"hidden_size\"],\n",
        "            num_layers=h_params[\"num_layers\"],\n",
        "            dropout=h_params[\"dropout\"] if h_params[\"num_layers\"] > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(h_params[\"dropout\"])\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        # Embed input\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        # Pass through RNN\n",
        "        output, hidden = self.rnn_cell(embedded, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize hidden state\"\"\"\n",
        "        h = torch.zeros(self.h_params[\"num_layers\"], batch_size,\n",
        "                      self.h_params[\"hidden_size\"], device=device)\n",
        "\n",
        "        if self.h_params[\"cell_type\"] == RNNType.LSTM:\n",
        "            c = torch.zeros(self.h_params[\"num_layers\"], batch_size,\n",
        "                          self.h_params[\"hidden_size\"], device=device)\n",
        "            return (h, c)\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder model for sequence-to-sequence learning\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embed_dim\"])\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn_cell = RNNType.get_cell(h_params[\"cell_type\"])(\n",
        "            input_size=h_params[\"char_embed_dim\"],\n",
        "            hidden_size=h_params[\"hidden_size\"],\n",
        "            num_layers=h_params[\"num_layers\"],\n",
        "            dropout=h_params[\"dropout\"] if h_params[\"num_layers\"] > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(h_params[\"hidden_size\"], data[\"target_len\"])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(h_params[\"dropout\"])\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        # Handle different input dimensions\n",
        "        if x.dim() == 0:  # scalar\n",
        "            x = x.unsqueeze(0).unsqueeze(0)  # (1, 1)\n",
        "        elif x.dim() == 1:  # (batch,)\n",
        "            x = x.unsqueeze(1)  # (batch, 1)\n",
        "\n",
        "        # Embed input\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        embedded = F.relu(embedded)\n",
        "\n",
        "        # Pass through RNN\n",
        "        output, hidden = self.rnn_cell(embedded, hidden)\n",
        "\n",
        "        # Get prediction\n",
        "        prediction = self.fc(output)\n",
        "\n",
        "        return F.log_softmax(prediction, dim=2), hidden\n",
        "\n",
        "    def greedy_decode(self, encoder_outputs, encoder_hidden, processor, max_length=None):\n",
        "        \"\"\"Greedy decoding method\"\"\"\n",
        "        # Prepare for decoding\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Storage for output\n",
        "        output_tokens = []\n",
        "\n",
        "        # Set max decoding length\n",
        "        if max_length is None:\n",
        "            max_length = self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "        # Greedy decoding\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Get best prediction\n",
        "            _, topi = decoder_output.data.topk(1)\n",
        "            predicted_idx = topi.squeeze().item()\n",
        "\n",
        "            # Stop if END token\n",
        "            if predicted_idx == self.data[\"target_char_index\"][END_TOKEN]:\n",
        "                break\n",
        "\n",
        "            # Add to outputs if it's a valid character\n",
        "            if predicted_idx in self.data[\"target_index_char\"]:\n",
        "                output_tokens.append(self.data[\"target_index_char\"][predicted_idx])\n",
        "\n",
        "            # Next input is predicted token\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        # Combine into output string\n",
        "        return ''.join(output_tokens)\n",
        "\n",
        "    def beam_search_decode(self, encoder_outputs, encoder_hidden, processor, beam_width=5, max_length=None):\n",
        "        \"\"\"Beam search decoding method\"\"\"\n",
        "        if max_length is None:\n",
        "            max_length = self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "        # Start with START_TOKEN\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Number of sentences to generate\n",
        "        end_nodes = []\n",
        "\n",
        "        # Starting node\n",
        "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
        "        nodes = [(-node.eval(), node)]  # Priority queue (lowest score first)\n",
        "\n",
        "        # Start beam search\n",
        "        for step in range(max_length):\n",
        "            # Give up when decoding takes too long\n",
        "            if len(nodes) == 0:\n",
        "                break\n",
        "\n",
        "            # Fetch the best node\n",
        "            score, current_node = heapq.heappop(nodes)\n",
        "            decoder_input = current_node.word_id\n",
        "            decoder_hidden = current_node.hidden_state\n",
        "\n",
        "            # If we reached the EOS token\n",
        "            if decoder_input.item() == self.data[\"target_char_index\"][END_TOKEN] and current_node.previous_node is not None:\n",
        "                end_nodes.append((score, current_node))\n",
        "                # If we have enough end nodes, stop\n",
        "                if len(end_nodes) >= beam_width:\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            # Decode for one step\n",
        "            decoder_output, decoder_hidden = self(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Get top-k tokens\n",
        "            log_probs, indexes = torch.topk(decoder_output.squeeze(), beam_width)\n",
        "\n",
        "            # Put them into the queue\n",
        "            for new_k in range(beam_width):\n",
        "                decoded_t = indexes[0][new_k].view(1)\n",
        "                log_p = log_probs[0][new_k].item()\n",
        "\n",
        "                # Create new node\n",
        "                node = BeamSearchNode(decoder_hidden, current_node, decoded_t, current_node.log_prob + log_p, current_node.length + 1)\n",
        "                heapq.heappush(nodes, (-node.eval(), node))\n",
        "\n",
        "        # If we don't have any end_nodes, get the top-k nodes from the queue\n",
        "        if len(end_nodes) == 0:\n",
        "            end_nodes = [heapq.heappop(nodes) for _ in range(min(beam_width, len(nodes)))]\n",
        "\n",
        "        # Get the best sequence\n",
        "        best_node = sorted(end_nodes, key=lambda x: x[0])[0][1]\n",
        "\n",
        "        # Traverse back to get the sequence\n",
        "        sequence = []\n",
        "        current = best_node\n",
        "        while current.previous_node is not None:\n",
        "            sequence.append(current.word_id.item())\n",
        "            current = current.previous_node\n",
        "\n",
        "        # Reverse the sequence and convert to string\n",
        "        sequence = sequence[::-1]\n",
        "        result = \"\"\n",
        "        for idx in sequence:\n",
        "            if idx in self.data[\"target_index_char\"] and idx != self.data[\"target_char_index\"][END_TOKEN]:\n",
        "                result += self.data[\"target_index_char\"][idx]\n",
        "\n",
        "        return result\n",
        "\n",
        "class Seq2SeqModel:\n",
        "    \"\"\"Wrapper for encoder-decoder architecture\"\"\"\n",
        "    def __init__(self, h_params, data):\n",
        "        self.encoder = Encoder(h_params, data).to(device)\n",
        "        self.decoder = Decoder(h_params, data).to(device)\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "\n",
        "    def forward(self, source, target=None, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"Forward pass through the model\"\"\"\n",
        "        batch_size = source.size(0)\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, encoder_hidden = self.encoder(source, encoder_hidden)\n",
        "\n",
        "        # Return early if no target (for inference)\n",
        "        if target is None:\n",
        "            return encoder_outputs, encoder_hidden\n",
        "\n",
        "        # Prepare decoder input and hidden state\n",
        "        decoder_input = torch.tensor([self.data[\"target_char_index\"][START_TOKEN]] * batch_size, device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        # Determine if using teacher forcing\n",
        "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "        # Storage for decoder outputs\n",
        "        decoder_outputs = []\n",
        "\n",
        "        # Decoder forward pass (one character at a time)\n",
        "        for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            # Determine next input (either from target or model's prediction)\n",
        "            if use_teacher_forcing and t < self.data[\"OUTPUT_MAX_LENGTH\"] - 1:\n",
        "                decoder_input = target[:, t + 1]  # Next input is next character in target\n",
        "            else:\n",
        "                # Get predicted character\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # Next input is model's prediction\n",
        "\n",
        "        # Stack decoder outputs\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "\n",
        "        return decoder_outputs, encoder_hidden\n",
        "\n",
        "    def predict(self, input_string, processor, use_beam_search=False, beam_width=5):\n",
        "        \"\"\"Make prediction for a single input string\"\"\"\n",
        "        # Prepare input\n",
        "        input_tensor = processor.prepare_input_for_prediction(input_string)\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        encoder_hidden = self.encoder.init_hidden(1)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "        # Decode\n",
        "        if use_beam_search:\n",
        "            prediction = self.decoder.beam_search_decode(encoder_outputs, encoder_hidden, processor, beam_width)\n",
        "        else:\n",
        "            prediction = self.decoder.greedy_decode(encoder_outputs, encoder_hidden, processor)\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "btw8s1uNOtyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    \"\"\"Class for training sequence-to-sequence models\"\"\"\n",
        "    def __init__(self, model, h_params, data, train_loader, val_loader):\n",
        "        self.model = model\n",
        "        self.h_params = h_params\n",
        "        self.data = data\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        # Initialize optimizers\n",
        "        if h_params[\"optimizer\"].lower() == \"adam\":\n",
        "            self.encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "        elif h_params[\"optimizer\"].lower() == \"nadam\":\n",
        "            self.encoder_optimizer = optim.NAdam(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.NAdam(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "        else:\n",
        "            self.encoder_optimizer = optim.SGD(model.encoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "            self.decoder_optimizer = optim.SGD(model.decoder.parameters(), lr=h_params[\"learning_rate\"])\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "        # Learning rate schedulers\n",
        "        self.encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.encoder_optimizer, 'min', patience=2)\n",
        "        self.decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.decoder_optimizer, 'min', patience=2)\n",
        "\n",
        "        # Best model tracking\n",
        "        self.best_val_accuracy = 0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.encoder.train()\n",
        "        self.model.decoder.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        total_correct = 0\n",
        "        total_examples = 0\n",
        "\n",
        "        for batch_idx, (source, target) in enumerate(self.train_loader):\n",
        "            batch_size = source.size(0)\n",
        "            if batch_size == 1:  # Skip batches with only one example\n",
        "                continue\n",
        "\n",
        "            # Zero gradients\n",
        "            self.encoder_optimizer.zero_grad()\n",
        "            self.decoder_optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                decoder_outputs, _ = self.model.forward(source, target, TEACHER_FORCING_RATIO)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = 0\n",
        "                all_predictions = []\n",
        "\n",
        "                for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "                    decoder_output = decoder_outputs[:, t, :]\n",
        "                    target_t = target[:, t]\n",
        "                    loss += self.criterion(decoder_output, target_t)\n",
        "\n",
        "                    # Get predicted character\n",
        "                    _, topi = decoder_output.topk(1)\n",
        "                    prediction = topi.squeeze().detach()\n",
        "                    all_predictions.append(prediction)\n",
        "\n",
        "                # Combine all predictions\n",
        "                predictions = torch.stack(all_predictions, dim=1)  # batch_size x seq_len\n",
        "\n",
        "                # Calculate accuracy (exact match)\n",
        "                correct = (predictions == target).all(dim=1).sum().item()\n",
        "\n",
        "                # Update totals\n",
        "                total_correct += correct\n",
        "                total_examples += batch_size\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.encoder.parameters(), 1)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.decoder.parameters(), 1)\n",
        "\n",
        "                # Update parameters\n",
        "                self.encoder_optimizer.step()\n",
        "                self.decoder_optimizer.step()\n",
        "\n",
        "                # Track loss\n",
        "                epoch_loss += loss.item() / self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "            # Free up memory\n",
        "            del source, target, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Run garbage collection periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Print progress periodically\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Batch {batch_idx + 1}/{len(self.train_loader)}, Loss: {epoch_loss/(batch_idx+1):.4f}\")\n",
        "\n",
        "        if total_examples == 0:\n",
        "            return 0, 0  # Avoid division by zero if all batches were skipped\n",
        "\n",
        "        return epoch_loss / len(self.train_loader), total_correct / total_examples\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        \"\"\"Evaluate model on validation or test data\"\"\"\n",
        "        self.model.encoder.eval()\n",
        "        self.model.decoder.eval()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        total_correct = 0\n",
        "        total_examples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (source, target) in enumerate(data_loader):\n",
        "                batch_size = source.size(0)\n",
        "                if batch_size == 1:  # Skip batches with only one example\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Forward pass\n",
        "                    decoder_outputs, _ = self.model.forward(source, target, teacher_forcing_ratio=1.0)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = 0\n",
        "                    all_predictions = []\n",
        "\n",
        "                    for t in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n",
        "                        decoder_output = decoder_outputs[:, t, :]\n",
        "                        target_t = target[:, t]\n",
        "                        loss += self.criterion(decoder_output, target_t)\n",
        "\n",
        "                        # Get predicted character\n",
        "                        _, topi = decoder_output.topk(1)\n",
        "                        prediction = topi.squeeze().detach()\n",
        "                        all_predictions.append(prediction)\n",
        "\n",
        "                    # Combine all predictions\n",
        "                    predictions = torch.stack(all_predictions, dim=1)  # batch_size x seq_len\n",
        "\n",
        "                    # Calculate accuracy (exact match)\n",
        "                    correct = (predictions == target).all(dim=1).sum().item()\n",
        "\n",
        "                    # Update totals\n",
        "                    total_correct += correct\n",
        "                    total_examples += batch_size\n",
        "                    epoch_loss += loss.item() / self.data[\"OUTPUT_MAX_LENGTH\"]\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Free up memory\n",
        "                del source, target\n",
        "\n",
        "                # Run garbage collection periodically\n",
        "                if batch_idx % 10 == 0:\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        if total_examples == 0:\n",
        "            return 0, 0  # Avoid division by zero if all batches were skipped\n",
        "\n",
        "        return total_correct / total_examples, epoch_loss / len(data_loader)\n",
        "\n",
        "    def train(self, test_source=None, test_target=None):\n",
        "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
        "        # WandB initialization\n",
        "        run_name = f\"{self.h_params['cell_type']}_{self.h_params['optimizer']}_noattn_layers{self.h_params['num_layers']}\"\n",
        "        try:\n",
        "            wandb.init(project=\"Tamil-Transliteration-NoAttn\", name=run_name, config=self.h_params)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing wandb: {e}\")\n",
        "            print(\"Continuing without wandb tracking...\")\n",
        "\n",
        "        for epoch in range(self.h_params[\"epochs\"]):\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.h_params['epochs']}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            # Train for one epoch\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_acc, val_loss = self.evaluate(self.val_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            self.encoder_scheduler.step(val_loss)\n",
        "            self.decoder_scheduler.step(val_loss)\n",
        "\n",
        "            # Log metrics\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            print(f\"Learning Rate: {self.encoder_optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"train_accuracy\": train_acc,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"val_accuracy\": val_acc,\n",
        "                    \"learning_rate\": self.encoder_optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_acc\n",
        "                self.save_model(f'best_model_{run_name}.pt', epoch, val_acc)\n",
        "\n",
        "            # Run garbage collection between epochs\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Test on test set if provided\n",
        "        if test_source is not None and test_target is not None:\n",
        "            self.test(test_source, test_target)\n",
        "\n",
        "        try:\n",
        "            wandb.finish()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def save_model(self, filename, epoch, val_accuracy):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        try:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'encoder_state_dict': self.model.encoder.state_dict(),\n",
        "                'decoder_state_dict': self.model.decoder.state_dict(),\n",
        "                'encoder_optimizer': self.encoder_optimizer.state_dict(),\n",
        "                'decoder_optimizer': self.decoder_optimizer.state_dict(),\n",
        "                'val_accuracy': val_accuracy,\n",
        "            }, filename)\n",
        "            print(f\"Model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model: {e}\")\n",
        "\n",
        "    def test(self, test_source, test_target):\n",
        "        \"\"\"Test model on test set\"\"\"\n",
        "        try:\n",
        "            print(\"Testing model on test set...\")\n",
        "\n",
        "            # Create DataProcessor for test data\n",
        "            processor = DataProcessor()\n",
        "            # Copy data dictionary\n",
        "            processor.data = copy.deepcopy(self.data)\n",
        "\n",
        "            # Process test data\n",
        "            test_source_seq, test_target_seq = processor.process_validation(test_source, test_target)\n",
        "            test_dataset = TransliterationDataset(test_source_seq, test_target_seq)\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=self.h_params[\"batch_size\"],\n",
        "                shuffle=False,\n",
        "                num_workers=0,\n",
        "                pin_memory=False\n",
        "            )\n",
        "\n",
        "            # Evaluate on test set\n",
        "            test_acc, test_loss = self.evaluate(test_loader)\n",
        "            print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "            # Log test metrics to wandb\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"test_loss\": test_loss,\n",
        "                    \"test_accuracy\": test_acc\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Generate predictions and compare with targets\n",
        "            evaluator = Evaluator(self.model, processor)\n",
        "            csv_path, results, csv_accuracy = evaluator.generate_predictions_csv(test_source, test_target)\n",
        "\n",
        "            # Log final metrics to wandb\n",
        "            try:\n",
        "                wandb.log({\n",
        "                    \"final_test_accuracy\": test_acc,\n",
        "                    \"final_test_loss\": test_loss,\n",
        "                    \"csv_accuracy\": csv_accuracy\n",
        "                })\n",
        "\n",
        "                # Create a wandb Table for sample predictions\n",
        "                prediction_table = wandb.Table(columns=[\"Input\", \"Target\", \"Prediction\", \"Correct\"])\n",
        "                for i in range(min(100, len(results))):\n",
        "                    correct = results[i]['Prediction'] == results[i]['Target']\n",
        "                    prediction_table.add_data(results[i]['Input'], results[i]['Target'],\n",
        "                                             results[i]['Prediction'], correct)\n",
        "\n",
        "                wandb.log({\"prediction_samples\": prediction_table})\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not log to wandb: {e}\")\n",
        "\n",
        "            # Display sample predictions\n",
        "            evaluator.visualize_samples(test_source, test_target, num_samples=10)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in testing: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"Class for evaluating transliteration models\"\"\"\n",
        "    def __init__(self, model, processor):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.data = processor.data\n",
        "\n",
        "    def predict(self, input_string, use_beam_search=False, beam_width=5):\n",
        "        \"\"\"Make prediction for a single input string\"\"\"\n",
        "        return self.model.predict(input_string, self.processor, use_beam_search, beam_width)\n",
        "\n",
        "    def generate_predictions_csv(self, test_source, test_target, use_beam_search=False):\n",
        "        \"\"\"Generate predictions for all test data and save to CSV\"\"\"\n",
        "        print(f\"Generating predictions for {len(test_source)} test examples...\")\n",
        "\n",
        "        results = []\n",
        "        # Generate predictions\n",
        "        for i in range(len(test_source)):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"Processing test example {i+1}/{len(test_source)}\")\n",
        "\n",
        "            input_str = test_source[i]\n",
        "            target_str = test_target[i]\n",
        "\n",
        "            # Get prediction\n",
        "            pred_str = self.predict(input_str, use_beam_search)\n",
        "\n",
        "            # Store result\n",
        "            results.append({\n",
        "                'Input': input_str,\n",
        "                'Prediction': pred_str,\n",
        "                'Target': target_str\n",
        "            })\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = sum(1 for r in results if r['Prediction'] == r['Target'])\n",
        "        accuracy = correct / len(results) if results else 0\n",
        "        print(f\"Test Accuracy: {accuracy:.4f} ({correct}/{len(results)})\")\n",
        "\n",
        "        # Add accuracy to wandb\n",
        "        try:\n",
        "            wandb.log({\"CSV_Test_Accuracy\": accuracy})\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Save to CSV\n",
        "        csv_path = 'tamil_transliteration_predictions.csv'\n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['Input', 'Prediction', 'Target']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for row in results:\n",
        "                writer.writerow(row)\n",
        "\n",
        "        print(f\"Saved predictions to {csv_path}\")\n",
        "\n",
        "        # Also save some metadata to help with file encoding issues\n",
        "        with open('encoding_info.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"File Encoding: UTF-8\\n\")\n",
        "            f.write(f\"Total Samples: {len(results)}\\n\")\n",
        "            f.write(f\"Correct Predictions: {correct}\\n\")\n",
        "            f.write(f\"Accuracy: {accuracy:.4f}\\n\\n\")\n",
        "            f.write(\"Sample Predictions (for debugging):\\n\")\n",
        "            for i in range(min(5, len(results))):\n",
        "                f.write(f\"Input: {results[i]['Input']}\\n\")\n",
        "                f.write(f\"Prediction: {results[i]['Prediction']}\\n\")\n",
        "                f.write(f\"Target: {results[i]['Target']}\\n\")\n",
        "                f.write(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "        return csv_path, results, accuracy\n",
        "\n",
        "    def visualize_samples(self, test_source, test_target, num_samples=10, use_beam_search=False):\n",
        "        \"\"\"Visualize sample predictions\"\"\"\n",
        "        samples_indices = random.sample(range(len(test_source)), min(num_samples, len(test_source)))\n",
        "\n",
        "        input_strings = []\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        for idx in samples_indices:\n",
        "            input_str = test_source[idx]\n",
        "            target_str = test_target[idx]\n",
        "\n",
        "            # Get prediction\n",
        "            pred_str = self.predict(input_str, use_beam_search)\n",
        "\n",
        "            input_strings.append(input_str)\n",
        "            predictions.append(pred_str)\n",
        "            targets.append(target_str)\n",
        "\n",
        "        # Display results\n",
        "        self.display_prediction_results(input_strings, predictions, targets)\n",
        "\n",
        "    def display_prediction_results(self, input_strings, predictions, targets, num_examples=None):\n",
        "        \"\"\"Display prediction results using HTML with web fonts\"\"\"\n",
        "        if num_examples is None:\n",
        "            num_examples = len(input_strings)\n",
        "\n",
        "        # Create HTML with Tamil web font\n",
        "        html = \"\"\"\n",
        "        <html>\n",
        "        <head>\n",
        "            <meta charset=\"UTF-8\">\n",
        "            <style>\n",
        "                @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Tamil:wght@400;700&display=swap');\n",
        "                body {\n",
        "                    font-family: 'Noto Sans Tamil', sans-serif;\n",
        "                    font-size: 16px;\n",
        "                    line-height: 1.6;\n",
        "                }\n",
        "                .result {\n",
        "                    margin-bottom: 20px;\n",
        "                    padding: 15px;\n",
        "                    border: 1px solid #ddd;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .correct { color: green; }\n",
        "                .incorrect { color: red; }\n",
        "                .input { font-weight: bold; }\n",
        "                .unicode { color: #888; font-size: 12px; font-family: monospace; }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <h2>Transliteration Results</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        # Limit examples\n",
        "        n = min(num_examples, len(input_strings))\n",
        "\n",
        "        for i in range(n):\n",
        "            input_str = input_strings[i]\n",
        "            pred = predictions[i]\n",
        "            target = targets[i]\n",
        "\n",
        "            correct = pred == target\n",
        "            status_class = \"correct\" if correct else \"incorrect\"\n",
        "\n",
        "            # Add this result to the HTML\n",
        "            html += f\"\"\"\n",
        "            <div class=\"result\">\n",
        "                <div class=\"input\">Input: {input_str}</div>\n",
        "                <div class=\"{status_class}\">Prediction: {pred}</div>\n",
        "                <div>Target: {target}</div>\n",
        "                <div class=\"unicode\">Unicode (Pred): {' '.join([f'U+{ord(c):04X}' for c in pred])}</div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"\"\"\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Display the HTML in the notebook\n",
        "        try:\n",
        "            display(HTML(html))\n",
        "        except:\n",
        "            # Fallback if IPython display is not available\n",
        "            print(\"HTML output not available, showing plain text results:\")\n",
        "            for i in range(n):\n",
        "                input_str = input_strings[i]\n",
        "                pred = predictions[i]\n",
        "                target = targets[i]\n",
        "                correct = pred == target\n",
        "                status = \"✓\" if correct else \"✗\"\n",
        "                print(f\"Sample {i+1} {status}\")\n",
        "                print(f\"Input: {input_str}\")\n",
        "                print(f\"Prediction: {pred}\")\n",
        "                print(f\"Target: {target}\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "        # Return statistics\n",
        "        correct_count = sum(1 for i in range(len(predictions)) if predictions[i] == targets[i])\n",
        "        return {\n",
        "            \"accuracy\": correct_count / len(predictions) if predictions else 0,\n",
        "            \"correct\": correct_count,\n",
        "            \"total\": len(predictions)\n",
        "        }"
      ],
      "metadata": {
        "id": "Q56M8uCkOyEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperparameterTuner:\n",
        "    \"\"\"Class for tuning hyperparameters using wandb sweeps\"\"\"\n",
        "    def __init__(self, data_manager):\n",
        "        self.data_manager = data_manager\n",
        "\n",
        "    def get_sweep_config(self):\n",
        "        \"\"\"Define hyperparameter sweep configuration\"\"\"\n",
        "        sweep_config = {\n",
        "            'method': 'bayes',\n",
        "            'name': 'tamil-transliteration-sweep',\n",
        "            'metric': {\n",
        "                'goal': 'maximize',\n",
        "                'name': 'val_accuracy'\n",
        "            },\n",
        "            'parameters': {\n",
        "                'learning_rate': {\n",
        "                    'values': [0.001, 0.0005, 0.0001]\n",
        "                },\n",
        "                'batch_size': {\n",
        "                    'values': [32, 64, 128, 256]\n",
        "                },\n",
        "                'char_embed_dim': {\n",
        "                    'values': [64, 128, 256]\n",
        "                },\n",
        "                'hidden_size': {\n",
        "                    'values': [128, 256, 512]\n",
        "                },\n",
        "                'num_layers': {\n",
        "                    'values': [1, 2, 3, 4]\n",
        "                },\n",
        "                'cell_type': {\n",
        "                    'values': [RNNType.RNN, RNNType.LSTM, RNNType.GRU]\n",
        "                },\n",
        "                'dropout': {\n",
        "                    'values': [0.0, 0.1, 0.2, 0.3]\n",
        "                },\n",
        "                'optimizer': {\n",
        "                    'values': ['adam', 'nadam']\n",
        "                },\n",
        "                'epochs': {\n",
        "                    'values': [10, 15, 20]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return sweep_config\n",
        "\n",
        "    def sweep_agent(self):\n",
        "        \"\"\"Sweep agent function\"\"\"\n",
        "        wandb.init()\n",
        "\n",
        "        # Access sweep config\n",
        "        config = wandb.config\n",
        "\n",
        "        # Define hyperparameters\n",
        "        h_params = {\n",
        "            \"char_embed_dim\": config.char_embed_dim,\n",
        "            \"hidden_size\": config.hidden_size,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"num_layers\": config.num_layers,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"epochs\": config.epochs,\n",
        "            \"cell_type\": config.cell_type,\n",
        "            \"dropout\": config.dropout,\n",
        "            \"optimizer\": config.optimizer\n",
        "        }\n",
        "\n",
        "        # Load data\n",
        "        try:\n",
        "            # Process data\n",
        "            data = self.data_manager.load_all_data()\n",
        "\n",
        "            # Create dataloaders\n",
        "            train_loader, val_loader = self.data_manager.create_dataloaders(h_params)\n",
        "\n",
        "            # Create model\n",
        "            model = Seq2SeqModel(h_params, data)\n",
        "\n",
        "            # Train model\n",
        "            trainer = Trainer(model, h_params, data, train_loader, val_loader)\n",
        "            trainer.train(self.data_manager.test_source, self.data_manager.test_target)\n",
        "\n",
        "            # Close wandb run\n",
        "            wandb.finish()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sweep agent: {e}\")\n",
        "            traceback.print_exc()\n",
        "            wandb.finish()\n",
        "\n",
        "    def run_sweep(self, count=10):\n",
        "        \"\"\"Run hyperparameter sweep\"\"\"\n",
        "        sweep_config = self.get_sweep_config()\n",
        "        sweep_id = wandb.sweep(sweep=sweep_config, project=\"Tamil-Transliteration-NoAttn\")\n",
        "        wandb.agent(sweep_id, function=self.sweep_agent, count=count)"
      ],
      "metadata": {
        "id": "49uwFs4SO1nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment:\n",
        "    \"\"\"Class for running experiments\"\"\"\n",
        "    def __init__(self):\n",
        "        # Default hyperparameters\n",
        "        self.default_hyperparams = {\n",
        "            \"char_embed_dim\": 256,\n",
        "            \"hidden_size\": 256,\n",
        "            \"batch_size\": 256,\n",
        "            \"num_layers\": 2,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"epochs\": 15,\n",
        "            \"cell_type\": RNNType.GRU,\n",
        "            \"dropout\": 0.2,\n",
        "            \"optimizer\": \"nadam\"\n",
        "        }\n",
        "\n",
        "    def run(self, use_sweep=False, num_sweep_runs=10, use_beam_search=False):\n",
        "        \"\"\"Run experiment\"\"\"\n",
        "        try:\n",
        "            # Initial CUDA memory reset\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Create data manager\n",
        "            data_manager = DataManager(DATA_PATHS)\n",
        "\n",
        "            if use_sweep:\n",
        "                # Run hyperparameter sweep\n",
        "                tuner = HyperparameterTuner(data_manager)\n",
        "                tuner.run_sweep(count=num_sweep_runs)\n",
        "            else:\n",
        "                # Load data\n",
        "                data = data_manager.load_all_data()\n",
        "\n",
        "                # Create dataloaders\n",
        "                train_loader, val_loader = data_manager.create_dataloaders(self.default_hyperparams)\n",
        "\n",
        "                # Create model\n",
        "                model = Seq2SeqModel(self.default_hyperparams, data)\n",
        "\n",
        "                # Initialize wandb\n",
        "                run_name = f\"{self.default_hyperparams['cell_type']}_{self.default_hyperparams['optimizer']}_noattn_layers{self.default_hyperparams['num_layers']}\"\n",
        "                try:\n",
        "                    wandb.init(project=\"Tamil-Transliteration-NoAttn\", name=run_name, config=self.default_hyperparams)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error initializing wandb: {e}\")\n",
        "\n",
        "                # Train model\n",
        "                print(f\"Starting training with {self.default_hyperparams['cell_type']} cell, {self.default_hyperparams['optimizer']} optimizer (No Attention)\")\n",
        "                trainer = Trainer(model, self.default_hyperparams, data, train_loader, val_loader)\n",
        "                trainer.train(data_manager.test_source, data_manager.test_target)\n",
        "\n",
        "                # Test with beam search if requested\n",
        "                if use_beam_search and model is not None:\n",
        "                    print(\"\\nTesting with beam search...\")\n",
        "                    processor = DataProcessor()\n",
        "                    processor.data = data\n",
        "                    evaluator = Evaluator(model, processor)\n",
        "\n",
        "                    # Compare greedy vs beam search predictions\n",
        "                    print(\"Comparing greedy vs beam search predictions...\")\n",
        "                    samples_indices = random.sample(range(len(data_manager.test_source)), 10)\n",
        "\n",
        "                    greedy_results = []\n",
        "                    beam_results = []\n",
        "\n",
        "                    for idx in samples_indices:\n",
        "                        input_str = data_manager.test_source[idx]\n",
        "                        target_str = data_manager.test_target[idx]\n",
        "\n",
        "                        # Get predictions\n",
        "                        greedy_pred = evaluator.predict(input_str, use_beam_search=False)\n",
        "                        beam_pred = evaluator.predict(input_str, use_beam_search=True, beam_width=5)\n",
        "\n",
        "                        greedy_results.append({\n",
        "                            'Input': input_str,\n",
        "                            'Prediction': greedy_pred,\n",
        "                            'Target': target_str\n",
        "                        })\n",
        "\n",
        "                        beam_results.append({\n",
        "                            'Input': input_str,\n",
        "                            'Prediction': beam_pred,\n",
        "                            'Target': target_str\n",
        "                        })\n",
        "\n",
        "                    # Display comparison\n",
        "                    print(\"\\nGreedy Search Results:\")\n",
        "                    greedy_inputs = [r['Input'] for r in greedy_results]\n",
        "                    greedy_preds = [r['Prediction'] for r in greedy_results]\n",
        "                    greedy_targets = [r['Target'] for r in greedy_results]\n",
        "                    evaluator.display_prediction_results(greedy_inputs, greedy_preds, greedy_targets)\n",
        "\n",
        "                    print(\"\\nBeam Search Results:\")\n",
        "                    beam_inputs = [r['Input'] for r in beam_results]\n",
        "                    beam_preds = [r['Prediction'] for r in beam_results]\n",
        "                    beam_targets = [r['Target'] for r in beam_results]\n",
        "                    evaluator.display_prediction_results(beam_inputs, beam_preds, beam_targets)\n",
        "\n",
        "                # Close wandb run\n",
        "                try:\n",
        "                    wandb.finish()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in experiment: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Run experiment\n",
        "def main():\n",
        "    experiment = Experiment()\n",
        "\n",
        "    # Set use_sweep=True to run hyperparameter sweep, False to run with default hyperparameters\n",
        "    # Set use_beam_search=True to test beam search decoding\n",
        "    experiment.run(use_sweep=False, use_beam_search=True)\n",
        "\n",
        "    # Uncomment to run sweep with 20 runs\n",
        "    # experiment.run(use_sweep=True, num_sweep_runs=20)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "33BtxBcxO5nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3WVLc-dO9QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}